{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "b_mnistTf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaishaliChetia/CapsNet-Keras/blob/master/c_mnistTf2100epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeoBe9bpMlPR"
      },
      "source": [
        "Original implementation at:\n",
        "\n",
        "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb\n",
        "\n",
        "Geron's model doesn't use the keras functional API. In the keras functional API, you don't need to give the batchsize. \n",
        "\n",
        "When you print the model, you get this:\n",
        "\n",
        "```\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "_________________________________________________________________\n",
        "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
        "_________________________________________________________________\n",
        "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
        "_________________________________________________________________\n",
        "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
        "_________________________________________________________________\n",
        "reshape_layer_1 (Reshape)    (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "caps1_output_layer (SquashLa (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "Total params: 5,329,664\n",
        "Trainable params: 5,329,664\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "Notice that the Input-layer has shape (None, 28, 28, 1), but we only specified (28, 28, 1). It added None implicitly and that takes care of the batch.\n",
        "\n",
        "So for anywhere Geron uses the batch size explicitly, you don't need to do anything and tensorflow will take care of.\n",
        "\n",
        "Also note that tensorflow 1 APIs are still provided with the compat layer. I used the reduce_sum from TF1 in the squash layer, that allowed me to use Geron's code.\n",
        "\n",
        "Documentation on how to migrate from TF1 to TF2 can be found here:\n",
        "\n",
        "https://www.tensorflow.org/guide/migrate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCmDd7lMlPU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UydL5gJMlPV"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "tf.random.set_seed(500000)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukwGEY4MlPV"
      },
      "source": [
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qIVI_u2i-s"
      },
      "source": [
        "class SafeNorm(K.layers.Layer):\n",
        "  \n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    self.keep_dims = keep_dims\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  def call(self, input):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims= self.keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECOObckMlPW"
      },
      "source": [
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1  # TODO: use\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\",\\\n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\\\n",
        "                      initializer=\"random_normal\",\\\n",
        "                      dtype=tf.float32)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def call(self, inputs):\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    return caps2_predicted"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg6qxAU3h0hv"
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    \n",
        "\n",
        "    def call(self,y_true, input):\n",
        "      # print(f\"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "      # return K.losses.MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "      #y_true = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "      m_plus = 0.9\n",
        "      m_minus = 0.1\n",
        "      lambda_ = 0.5 \n",
        "      \n",
        "      #y_true one hot encode y_train\n",
        "      T = tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      \n",
        "      caps2_output_norm = self.safe_norm(input, keep_dims = True)\n",
        "\n",
        "      present_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                                    name=\"present_error_raw\")\n",
        "      present_error = tf.reshape(\\\n",
        "                                    present_error_raw, shape=(-1, 10),\n",
        "                                    name=\"present_error\")  \n",
        "  \n",
        "      absent_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                                    name=\"absent_error_raw\")\n",
        "      absent_error = tf.reshape(\\\n",
        "                                    absent_error_raw, shape=(-1, 10),\n",
        "                                    name=\"absent_error\")\n",
        "  \n",
        "      L = tf.add(\\\n",
        "                  T * present_error,\\\n",
        "                  lambda_ * (1.0 - T) * absent_error,\n",
        "                  name=\"L\")\n",
        "      marginLoss = tf.reduce_mean(\\\n",
        "                                  tf.reduce_sum(L, axis=1),\\\n",
        "                                  name=\"margin_loss\")\n",
        "      return marginLoss\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXMBYOeWlDd"
      },
      "source": [
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, roundno=-1, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = roundno\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def squash(inputs):\n",
        "    EPSILON = 1.0e-7\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def single_round_routing(self, inputs, raw_weights, agreement):\n",
        "    raw_weights = tf.add(raw_weights, agreement)\n",
        "    routing_wt = tf.nn.softmax(raw_weights, axis=2)\n",
        "    wt_predictions = tf.multiply(routing_wt, inputs)\n",
        "    wt_sum = tf.reduce_sum(wt_predictions, axis=1, keepdims=True)\n",
        "    return wt_sum\n",
        "\n",
        "  def call(self, inputs):\n",
        "    agreement = tf.zeros(shape=self.raw_weights_1.shape)\n",
        "    sqsh_wt_sum = None\n",
        "    x = inputs\n",
        "    for i in range(2):\n",
        "      wt_sum = self.single_round_routing(inputs, self.raw_weights_1, agreement)\n",
        "      sqsh_wt_sum = RoutingByAgreement.squash(wt_sum)\n",
        "      sqsh_wt_sum_tiled = tf.tile(\\\n",
        "                          sqsh_wt_sum ,\\\n",
        "                          [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                          name=\"caps2_output_round_1_tiled\")\n",
        "      agreement = tf.matmul(\\\n",
        "                            x, \\\n",
        "                            sqsh_wt_sum_tiled,\\\n",
        "                            transpose_a=True,\\\n",
        "                            name=\"agreement\")\n",
        "    return sqsh_wt_sum                           \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEe-231jn49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fead0833-1f5a-4e86-d641-9a749b03de6c"
      },
      "source": [
        "(x_train, y_train,), (x_test, y_test) = K.datasets.mnist.load_data()\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmSudqTMlPX",
        "outputId": "e05e92f4-2560-4510-961d-676ea90701e7"
      },
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def build(inshape=(28, 28, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape(\\\n",
        "                             (caps1_n_caps, caps1_n_dims),\\\n",
        "                             name=\"reshape_layer_1\")(x)\n",
        "                             \n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement (2 rounds)\n",
        "        x = RoutingByAgreement(name=\"routing1\", roundno=2)(x)\n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "print(m.summary())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3j_zWQMlPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eea67ed-b6b9-4f33-b663-4e36d17a77f1"
      },
      "source": [
        "y_train_train = tf.one_hot(y_train, depth=caps2_n_caps, name=\"T\")\n",
        "print(y_train_train.shape)\n",
        "#print(y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zxRXCYIz_HQ"
      },
      "source": [
        "class MyAccuracy(K.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyAccuracy, self).__init__(**kwargs)\n",
        "    self.acc_obj = K.metrics.Accuracy()\n",
        "    self.state = 0\n",
        "  \n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def update_state(self, y_true, input, sample_weight=None):\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    y_true = tf.reshape(y_true, (y_true.shape[0], ))\n",
        "    y_true = tf.cast(y_true, dtype=tf.int64)\n",
        "    self.acc_obj.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.acc_obj.reset_state()\n",
        "\n",
        "  def result(self):\n",
        "    return self.acc_obj.result()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cahMgtOqMlPa",
        "outputId": "75c46ef3-1126-4d3a-ce2a-400a386ce034"
      },
      "source": [
        "m.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "history = m.fit(x_train, y_train, batch_size=32, epochs=100, verbose= 1, validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 130s 84ms/step - loss: 2.2811 - my_accuracy: 0.2528 - val_loss: 2.2802 - val_my_accuracy: 0.4931\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 125s 83ms/step - loss: 2.2799 - my_accuracy: 0.4912 - val_loss: 2.2804 - val_my_accuracy: 0.6408\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2796 - my_accuracy: 0.5906 - val_loss: 2.2795 - val_my_accuracy: 0.8343\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 125s 83ms/step - loss: 2.2794 - my_accuracy: 0.6519 - val_loss: 2.2793 - val_my_accuracy: 0.8512\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 125s 84ms/step - loss: 2.2792 - my_accuracy: 0.7005 - val_loss: 2.2799 - val_my_accuracy: 0.7839\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 125s 83ms/step - loss: 2.2792 - my_accuracy: 0.7571 - val_loss: 2.2797 - val_my_accuracy: 0.7117\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2791 - my_accuracy: 0.7587 - val_loss: 2.2799 - val_my_accuracy: 0.8142\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2790 - my_accuracy: 0.7947 - val_loss: 2.2801 - val_my_accuracy: 0.7648\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2791 - my_accuracy: 0.7969 - val_loss: 2.2795 - val_my_accuracy: 0.8556\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2790 - my_accuracy: 0.8141 - val_loss: 2.2797 - val_my_accuracy: 0.6068\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2790 - my_accuracy: 0.8069 - val_loss: 2.2794 - val_my_accuracy: 0.9323\n",
            "Epoch 12/100\n",
            "1500/1500 [==============================] - 117s 78ms/step - loss: 2.2790 - my_accuracy: 0.8186 - val_loss: 2.2803 - val_my_accuracy: 0.7051\n",
            "Epoch 13/100\n",
            "1500/1500 [==============================] - 118s 78ms/step - loss: 2.2790 - my_accuracy: 0.8169 - val_loss: 2.2797 - val_my_accuracy: 0.8682\n",
            "Epoch 14/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2789 - my_accuracy: 0.8056 - val_loss: 2.2794 - val_my_accuracy: 0.8339\n",
            "Epoch 15/100\n",
            "1500/1500 [==============================] - 125s 84ms/step - loss: 2.2789 - my_accuracy: 0.8374 - val_loss: 2.2796 - val_my_accuracy: 0.8439\n",
            "Epoch 16/100\n",
            "1500/1500 [==============================] - 125s 84ms/step - loss: 2.2789 - my_accuracy: 0.8322 - val_loss: 2.2795 - val_my_accuracy: 0.6147\n",
            "Epoch 17/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2789 - my_accuracy: 0.8416 - val_loss: 2.2795 - val_my_accuracy: 0.6977\n",
            "Epoch 18/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2789 - my_accuracy: 0.8284 - val_loss: 2.2793 - val_my_accuracy: 0.8259\n",
            "Epoch 19/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2788 - my_accuracy: 0.8355 - val_loss: 2.2796 - val_my_accuracy: 0.9241\n",
            "Epoch 20/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2789 - my_accuracy: 0.8639 - val_loss: 2.2798 - val_my_accuracy: 0.7031\n",
            "Epoch 21/100\n",
            "1500/1500 [==============================] - 125s 83ms/step - loss: 2.2788 - my_accuracy: 0.8416 - val_loss: 2.2796 - val_my_accuracy: 0.8553\n",
            "Epoch 22/100\n",
            "1500/1500 [==============================] - 125s 84ms/step - loss: 2.2788 - my_accuracy: 0.8340 - val_loss: 2.2794 - val_my_accuracy: 0.8493\n",
            "Epoch 23/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2788 - my_accuracy: 0.8479 - val_loss: 2.2794 - val_my_accuracy: 0.8988\n",
            "Epoch 24/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2788 - my_accuracy: 0.8546 - val_loss: 2.2796 - val_my_accuracy: 0.9416\n",
            "Epoch 25/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2788 - my_accuracy: 0.8323 - val_loss: 2.2793 - val_my_accuracy: 0.9013\n",
            "Epoch 26/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2787 - my_accuracy: 0.8337 - val_loss: 2.2795 - val_my_accuracy: 0.9114\n",
            "Epoch 27/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8705 - val_loss: 2.2795 - val_my_accuracy: 0.7396\n",
            "Epoch 28/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2788 - my_accuracy: 0.8374 - val_loss: 2.2792 - val_my_accuracy: 0.9118\n",
            "Epoch 29/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8620 - val_loss: 2.2795 - val_my_accuracy: 0.8342\n",
            "Epoch 30/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8620 - val_loss: 2.2793 - val_my_accuracy: 0.8880\n",
            "Epoch 31/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8551 - val_loss: 2.2794 - val_my_accuracy: 0.8839\n",
            "Epoch 32/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8646 - val_loss: 2.2793 - val_my_accuracy: 0.8666\n",
            "Epoch 33/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8628 - val_loss: 2.2794 - val_my_accuracy: 0.9266\n",
            "Epoch 34/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8441 - val_loss: 2.2793 - val_my_accuracy: 0.9143\n",
            "Epoch 35/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2788 - my_accuracy: 0.8461 - val_loss: 2.2795 - val_my_accuracy: 0.8638\n",
            "Epoch 36/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8375 - val_loss: 2.2795 - val_my_accuracy: 0.7769\n",
            "Epoch 37/100\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 2.2788 - my_accuracy: 0.8576 - val_loss: 2.2791 - val_my_accuracy: 0.9454\n",
            "Epoch 38/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8778 - val_loss: 2.2795 - val_my_accuracy: 0.9252\n",
            "Epoch 39/100\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 2.2787 - my_accuracy: 0.8828 - val_loss: 2.2793 - val_my_accuracy: 0.8577\n",
            "Epoch 40/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8623 - val_loss: 2.2792 - val_my_accuracy: 0.9429\n",
            "Epoch 41/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8682 - val_loss: 2.2794 - val_my_accuracy: 0.8970\n",
            "Epoch 42/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8338 - val_loss: 2.2793 - val_my_accuracy: 0.8380\n",
            "Epoch 43/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8365 - val_loss: 2.2794 - val_my_accuracy: 0.9353\n",
            "Epoch 44/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8618 - val_loss: 2.2793 - val_my_accuracy: 0.8512\n",
            "Epoch 45/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8612 - val_loss: 2.2794 - val_my_accuracy: 0.8088\n",
            "Epoch 46/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8435 - val_loss: 2.2791 - val_my_accuracy: 0.9392\n",
            "Epoch 47/100\n",
            "1500/1500 [==============================] - 129s 86ms/step - loss: 2.2788 - my_accuracy: 0.8860 - val_loss: 2.2792 - val_my_accuracy: 0.8434\n",
            "Epoch 48/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2788 - my_accuracy: 0.8426 - val_loss: 2.2793 - val_my_accuracy: 0.8407\n",
            "Epoch 49/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8490 - val_loss: 2.2800 - val_my_accuracy: 0.4233\n",
            "Epoch 50/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8535 - val_loss: 2.2791 - val_my_accuracy: 0.9091\n",
            "Epoch 51/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2786 - my_accuracy: 0.8330 - val_loss: 2.2791 - val_my_accuracy: 0.9625\n",
            "Epoch 52/100\n",
            "1500/1500 [==============================] - 127s 84ms/step - loss: 2.2788 - my_accuracy: 0.8764 - val_loss: 2.2793 - val_my_accuracy: 0.8737\n",
            "Epoch 53/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8660 - val_loss: 2.2793 - val_my_accuracy: 0.9094\n",
            "Epoch 54/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8682 - val_loss: 2.2793 - val_my_accuracy: 0.8729\n",
            "Epoch 55/100\n",
            "1500/1500 [==============================] - 118s 79ms/step - loss: 2.2787 - my_accuracy: 0.8608 - val_loss: 2.2793 - val_my_accuracy: 0.8504\n",
            "Epoch 56/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8718 - val_loss: 2.2794 - val_my_accuracy: 0.9145\n",
            "Epoch 57/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8740 - val_loss: 2.2793 - val_my_accuracy: 0.8935\n",
            "Epoch 58/100\n",
            "1500/1500 [==============================] - 126s 84ms/step - loss: 2.2788 - my_accuracy: 0.8622 - val_loss: 2.2792 - val_my_accuracy: 0.9351\n",
            "Epoch 59/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8511 - val_loss: 2.2793 - val_my_accuracy: 0.9153\n",
            "Epoch 60/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8375 - val_loss: 2.2793 - val_my_accuracy: 0.9197\n",
            "Epoch 61/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8447 - val_loss: 2.2792 - val_my_accuracy: 0.9321\n",
            "Epoch 62/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2788 - my_accuracy: 0.8638 - val_loss: 2.2791 - val_my_accuracy: 0.9134\n",
            "Epoch 63/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8686 - val_loss: 2.2794 - val_my_accuracy: 0.8388\n",
            "Epoch 64/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8644 - val_loss: 2.2795 - val_my_accuracy: 0.7967\n",
            "Epoch 65/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8865 - val_loss: 2.2797 - val_my_accuracy: 0.6861\n",
            "Epoch 66/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8764 - val_loss: 2.2794 - val_my_accuracy: 0.7832\n",
            "Epoch 67/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8791 - val_loss: 2.2794 - val_my_accuracy: 0.8777\n",
            "Epoch 68/100\n",
            "1500/1500 [==============================] - 127s 85ms/step - loss: 2.2787 - my_accuracy: 0.8671 - val_loss: 2.2793 - val_my_accuracy: 0.9266\n",
            "Epoch 69/100\n",
            "1500/1500 [==============================] - 128s 86ms/step - loss: 2.2787 - my_accuracy: 0.8724 - val_loss: 2.2792 - val_my_accuracy: 0.8941\n",
            "Epoch 70/100\n",
            "1500/1500 [==============================] - 128s 85ms/step - loss: 2.2787 - my_accuracy: 0.8491 - val_loss: 2.2794 - val_my_accuracy: 0.8133\n",
            "Epoch 71/100\n",
            " 444/1500 [=======>......................] - ETA: 1:15 - loss: 2.2784 - my_accuracy: 0.8613"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyUFx-dcMlPa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (30, 10)\n",
        "plt.rcParams[\"font.size\"] = 20\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].plot(history.history['my_accuracy'])\n",
        "ax[0].plot(history.history['val_my_accuracy'])\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].legend(['Training Accuracy', 'Validation Accuracy'], loc='best')\n",
        "\n",
        "ax[1].plot(history.history['loss'])\n",
        "ax[1].plot(history.history['val_loss'])\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend(['Training Loss', 'Validation Loss'], loc='best')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bnBQabNNKLH"
      },
      "source": [
        "print(f'Best Validation Accuracy = {np.max(history.history[\"val_my_accuracy\"])}')\n",
        "print(f'Best Training   Accuracy = {np.max(history.history[\"my_accuracy\"])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdMFENewNgFc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}