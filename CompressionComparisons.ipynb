{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "CompressionComparisons.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaishaliChetia/CapsNet-Keras/blob/master/CompressionComparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project subjects the Capsule network and a VGG16 (convolutional nueral network) to model optimiztion techniques such a pruning and quantization and comparest them with respect to Latency , Accuracy and Model Size. "
      ],
      "metadata": {
        "id": "QQmDcBBoAVic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Proessor Info"
      ],
      "metadata": {
        "id": "q_eMPWfkAOcN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwZSc0_9Pg7G",
        "outputId": "c903d85c-f8af-4b5e-e0ed-08cacd089983"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2000.128\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4000.25\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2000.128\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4000.25\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlZb-DbPqWB",
        "outputId": "ac78e750-37d0-420b-c7f9-e4f76492db35"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         148G   39G  110G  26% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/sda1       154G   41G  114G  27% /opt/bin/.nvidia\n",
            "tmpfs           6.4G   36K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F-fvMREQvXk",
        "outputId": "4f6456f8-c80e-40a2-c789-30d13491bd33"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemTotal:       13302924 kB\n",
            "MemFree:        10359412 kB\n",
            "MemAvailable:   12456960 kB\n",
            "Buffers:          146316 kB\n",
            "Cached:          2091884 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1039664 kB\n",
            "Inactive:        1608108 kB\n",
            "Active(anon):     379420 kB\n",
            "Inactive(anon):      484 kB\n",
            "Active(file):     660244 kB\n",
            "Inactive(file):  1607624 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:             18692 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        409588 kB\n",
            "Mapped:           237836 kB\n",
            "Shmem:              1180 kB\n",
            "KReclaimable:     142312 kB\n",
            "Slab:             193924 kB\n",
            "SReclaimable:     142312 kB\n",
            "SUnreclaim:        51612 kB\n",
            "KernelStack:        4992 kB\n",
            "PageTables:         5552 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6651460 kB\n",
            "Committed_AS:    3027928 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       47184 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1424 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      140096 kB\n",
            "DirectMap2M:     5099520 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKf1mS-tRcsd",
        "outputId": "200beb7c-a7d5-4e63-e51d-254b0c33d22a"
      },
      "source": [
        "!nvidia-smi\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!df -h\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!free -m\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!lscpu\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "! ps -eo pmem,pcpu,vsize,pid,cmd | sort -k 1 -nr | head -5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug 25 09:42:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         148G   38G  110G  26% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/sda1       154G   41G  114G  27% /opt/bin/.nvidia\n",
            "tmpfs           6.4G   36K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          12991         550       10114           1        2325       12164\n",
            "Swap:             0           0           0\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               85\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Stepping:            3\n",
            "CPU MHz:             2000.128\n",
            "BogoMIPS:            4000.25\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            1024K\n",
            "L3 cache:            39424K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            " 0.8 10.0 691784      74 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-895bdc1e-aab1-4300-89e0-1dace96accb4.json\n",
            " 0.4  2.7 193892      63 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n",
            " 0.3  1.6 337764       1 /tools/node/bin/node /datalab/web/app.js\n",
            " 0.1  0.2 127384      94 /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/debugpy/adapter --for-server 39285 --host 127.0.0.1 --port 18114 --server-access-token 6efb60f96b13235f7c912a6a039c1651813034e267ad3514d127ff9441d46390\n",
            "%MEM %CPU    VSZ     PID CMD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY0OS1x1qLp2",
        "outputId": "ae66cde0-ebb8-4be2-80ec-2e19ecd5ce54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCmDd7lMlPU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K\n",
        "#import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ1DIPu4B-Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db77958-d61a-4cb7-bdf6-9c86c5f395af"
      },
      "source": [
        "pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 5.1 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQG4QHY-CKYm"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI7eja-ysd9r"
      },
      "source": [
        "# Capsule Network for 28X28 image size\n",
        "first we build a capsule network and train it on mnist dataset with image size, and subject it to compression and check its performance with respect to accuracy, latency and model size. \n",
        "Please check the original code on Capsule network for detailed description of each function \n",
        "https://github.com/BaishaliChetia/MyProjects/blob/2452391b1dc6699979d1928c75d01fcf24aa6f9b/DeepLearning/CapsuleNetwork_pruned_quantized.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UydL5gJMlPV"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules , for 28X28 image size\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "tf.random.set_seed(500000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukwGEY4MlPV"
      },
      "source": [
        "#class SquashLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SquashLayer, self).get_config()\n",
        "    config.update({\"axis\": self.axis})\n",
        "    return config\n",
        "\n",
        " \n",
        "#class SafeNorm(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SafeNorm(K.layers.Layer):\n",
        "  \n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    self.keep_dims = keep_dims\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, input):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims= self.keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SafeNorm, self).get_config()\n",
        "    config.update({\"axis\": self.axis, \"keep_dims\": self.keep_dims})\n",
        "    return config\n",
        "  \n",
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "  \n",
        "  def get_config(self):\n",
        "    config =  super(MyDigitCapsLayer, self).get_config()\n",
        "    return config\n",
        "  \n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1  # TODO: use\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\",\\\n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\\\n",
        "                      initializer=\"random_normal\",\\\n",
        "                      dtype=tf.float32)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def get_prunable_weights(self):\n",
        "    return [self.kernel]\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    return caps2_predicted\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "\n",
        "  \n",
        "    def get_config(self):\n",
        "      config = super(MarginLoss, self).get_config()\n",
        "      return config\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    \"\"\"\n",
        "    def get_prunable_weights(self):\n",
        "      return []\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self,y_true, input):\n",
        "      # print(f\"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "      # return K.losses.MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "      #y_true = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "      m_plus = 0.9\n",
        "      m_minus = 0.1\n",
        "      lambda_ = 0.5 \n",
        "      \n",
        "      #y_true one hot encode y_train\n",
        "      T = tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      \n",
        "      caps2_output_norm = self.safe_norm(input, keep_dims = True)\n",
        "\n",
        "      present_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                                    name=\"present_error_raw\")\n",
        "      present_error = tf.reshape(\\\n",
        "                                    present_error_raw, shape=(-1, 10),\n",
        "                                    name=\"present_error\")  \n",
        "  \n",
        "      absent_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                                    name=\"absent_error_raw\")\n",
        "      absent_error = tf.reshape(\\\n",
        "                                    absent_error_raw, shape=(-1, 10),\n",
        "                                    name=\"absent_error\")\n",
        "  \n",
        "      L = tf.add(\\\n",
        "                  T * present_error,\\\n",
        "                  lambda_ * (1.0 - T) * absent_error,\n",
        "                  name=\"L\")\n",
        "      marginLoss = tf.reduce_mean(\\\n",
        "                                  tf.reduce_sum(L, axis=1),\\\n",
        "                                  name=\"margin_loss\")\n",
        "      return marginLoss\n",
        "\n",
        "\n",
        "#class RoutingByAgreement(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, round_number=-1, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = round_number \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(RoutingByAgreement, self).get_config()\n",
        "    config.update({\"round_number\": self.round_number})\n",
        "    return config\n",
        " \n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "  \n",
        "  def get_prunable_weights(self):\n",
        "    return [self.raw_weights_1]\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def squash(inputs):\n",
        "    EPSILON = 1.0e-5\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def single_round_routing(self, inputs, raw_weights, agreement):\n",
        "    raw_weights = tf.add(raw_weights, agreement)\n",
        "    routing_wt = tf.nn.softmax(raw_weights, axis=2)\n",
        "    wt_predictions = tf.multiply(routing_wt, inputs)\n",
        "    wt_sum = tf.reduce_sum(wt_predictions, axis=1, keepdims=True)\n",
        "    return wt_sum\n",
        "\n",
        "  def call(self, inputs):\n",
        "    agreement = tf.zeros(shape=self.raw_weights_1.shape)\n",
        "    sqsh_wt_sum = None\n",
        "    x = inputs\n",
        "    for i in range(2):\n",
        "      wt_sum = self.single_round_routing(inputs, self.raw_weights_1, agreement)\n",
        "      sqsh_wt_sum = RoutingByAgreement.squash(wt_sum)\n",
        "      sqsh_wt_sum_tiled = tf.tile(\\\n",
        "                          sqsh_wt_sum ,\\\n",
        "                          [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                          name=\"caps2_output_round_1_tiled\")\n",
        "      agreement = tf.matmul(\\\n",
        "                            x, \\\n",
        "                            sqsh_wt_sum_tiled,\\\n",
        "                            transpose_a=True,\\\n",
        "                            name=\"agreement\")\n",
        "    return sqsh_wt_sum\n",
        "\n",
        "class MyAccuracy(K.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyAccuracy, self).__init__(**kwargs)\n",
        "    self.acc_obj = K.metrics.Accuracy()\n",
        "    self.state = 0\n",
        "  \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyAccuracy, self).get_config()\n",
        "    config.update({\"acc_obj\": None, \"state\": self.state})\n",
        "    return config\n",
        "  \n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def update_state(self, y_true, input, sample_weight=None):\n",
        "    if self.acc_obj is None:\n",
        "      self.acc_obj = K.metrics.Accuracy()\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    #y_true = tf.reshape(y_true, (y_true.shape[0], ))\n",
        "    y_true = tf.cast(y_true, dtype=tf.int64)\n",
        "    self.acc_obj.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.acc_obj.reset_state()\n",
        "\n",
        "  def result(self):\n",
        "    return self.acc_obj.result()\n",
        "\n",
        "class MyReshapeLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(MyReshapeLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def call(self, input):\n",
        "    print('printing shapes ------------------- ')\n",
        "    EPSILON = 1.0e-9\n",
        "    print(input)\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    print(y_proba)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    print(y_proba_argmax)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    print(y_pred)\n",
        "    return tf.cast(y_pred, tf.int64)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyReshapeLayer, self).get_config()\n",
        "    return config\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEe-231jn49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42cb075-326f-4958-8fbf-5cc32131aef7"
      },
      "source": [
        "(x_train, y_train,), (x_test, y_test) = K.datasets.mnist.load_data()\n",
        "#print(x_train.shape, x_test.shape)\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmSudqTMlPX",
        "outputId": "bcb1ff4d-78d0-490e-8a58-1297cbc981af"
      },
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def build(inshape=(28, 28, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape(\\\n",
        "                             (caps1_n_caps, caps1_n_dims),\\\n",
        "                             name=\"reshape_layer_1\")(x)\n",
        "                             \n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement (2 rounds)\n",
        "        x = RoutingByAgreement(name=\"routing1\", round_number=2)(x)\n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "print(m.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZafeblGIsmD"
      },
      "source": [
        "m.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2sG4Xnds8MD"
      },
      "source": [
        "m.load_weights(\"/content/drive/MyDrive/MnistResults/best_weights4.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqR3akCBtPL3",
        "outputId": "800b3fb7-2556-4680-c13a-55124b6b6905"
      },
      "source": [
        "_, baseline_model_accuracy = m.evaluate(\n",
        "    x_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 26s 33ms/step - loss: 2.2766 - my_accuracy: 0.9837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "eolMpGZWiJvC",
        "outputId": "f9aec697-0621-406a-c985-3a9cda2a2ab4"
      },
      "source": [
        "y_pred = m.predict(x_test)\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"accuracy = {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a9876031313d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'newmodel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiR56NWSt4Sc",
        "outputId": "3ed864d4-2565-439b-8ce0-02b269225e36"
      },
      "source": [
        "print(\"For 28X28 image, evaluation time per test instance: \" + str((26/x_test.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For 28X28 image, evaluation time per test instance: 2.6ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCBHonuCHLvR"
      },
      "source": [
        "# class TimeHistory(K.callbacks.Callback):\n",
        "#     def on_train_begin(self, logs={}):\n",
        "#         self.times = []\n",
        "\n",
        "#     def on_epoch_begin(self, batch, logs={}):\n",
        "#         self.epoch_time_start = time.time()\n",
        "\n",
        "#     def on_epoch_end(self, batch, logs={}):\n",
        "#         self.times.append(time.time() - self.epoch_time_start)\n",
        "# time_callback = TimeHistory()\n",
        "# history = m.fit(x_train, y_train, batch_size=32, epochs=50, verbose= 1, validation_split=0.2, callbacks = [time_callback])\n",
        "# times = time_callback.times\n",
        "# print(times)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaHw8ou_DdfD"
      },
      "source": [
        "# t1 = time.perf_counter()\n",
        "# for i in range(x_test.shape[0]):\n",
        "#   y_pred[i] = newmodel.predict(x_test[i])\n",
        "#   print(f\"accuracy = {accuracy_score(y_test[i], y_pred[i])}\")\n",
        "# t2 = time.perf_counter()\n",
        "# per_instance_time = (t2-t1)/x_test.shape[0]\n",
        "# print(per_instance_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pOcuSJGSvLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e6f7dc-2644-4816-b15e-16ccbece03d3"
      },
      "source": [
        "#m.save(\"/content/drive/MyDrive/WeightsMnist/save.tf\", save_format='tf')\n",
        "\n",
        "# \n",
        "#!rm -f \"/content/drive/MyDrive/WeightsMnist/save3.tf\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/WeightsMnist/save3.tf': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SM-gwDR2FtA",
        "outputId": "e9c4f611-19c0-4e38-f483-cf8fa586bbb5"
      },
      "source": [
        "basemodel_file = m.save(\"/content/drive/MyDrive/MnistResults/caps_basemodelB.tf\", save_format='tf')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/caps_basemodelB.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR65DHK77BXQ"
      },
      "source": [
        "# model = tf.keras.models.load_model(\"/content/drive/MyDrive/MnistResults/save_basemodel5.tf\", \n",
        "#                                    custom_objects=\\\n",
        "#                           {\\\n",
        "#                               \"SquashLayer\": SquashLayer,\\\n",
        "#                               \"SafeNorm\": SafeNorm,\\\n",
        "#                               \"MyDigitCapsLayer\": MyDigitCapsLayer,\n",
        "#                               \"RoutingByAgreement\": RoutingByAgreement,                      \n",
        "#                               \"MyAccuracy\": MyAccuracy,                            \n",
        "#                               \"MarginLoss\": MarginLoss\n",
        "#                            })\n",
        "\n",
        "\n",
        "mm = K.models.load_model('/content/drive/MyDrive/WeightsMnist/save2.tf',\\\n",
        "                         custom_objects=\\\n",
        "                          {\\\n",
        "                              \"SquashLayer\": SquashLayer,\\\n",
        "                              \"SafeNorm\": SafeNorm,\\\n",
        "                              \"MyDigitCapsLayer\": MyDigitCapsLayer,\\\n",
        "                              \"RoutingByAgreement\": RoutingByAgreement,\\\n",
        "                              \"MyAccuracy\": MyAccuracy,\\\n",
        "                              \"MarginLoss\": MarginLoss,\\\n",
        "                          })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJLicHf3J1G-"
      },
      "source": [
        "# mm = K.models.load_model('/content/drive/MyDrive/WeightsMnist/save2.tf',\\\n",
        "#                          custom_objects=\\\n",
        "#                           {\\\n",
        "#                               \"SquashLayer\": SquashLayer,\\\n",
        "#                               \"SafeNorm\": SafeNorm,\\\n",
        "#                               \"MyDigitCapsLayer\": MyDigitCapsLayer,\\\n",
        "#                               \"RoutingByAgreement\": RoutingByAgreement,\\\n",
        "#                               \"MyAccuracy\": MyAccuracy,\\\n",
        "#                               \"MarginLoss\": MarginLoss,\\\n",
        "#                           })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-vE6yGAoBPI"
      },
      "source": [
        "# m3 = K.models.Sequential()\n",
        "# m3.add(m2)\n",
        "# m3.add(y_pred_eval)\n",
        "# m3.build()\n",
        "# m3.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "# m3.evaluate(x_test, y_test, batch_size= 32, verbose= 1)\n",
        "\n",
        "# #m3.evaluate(x_test, y_test, batch_size= 32, verbose= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vf-xeFUcgi"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Dq_06YHPJn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuULPnldermd"
      },
      "source": [
        "mm = m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdrYq9SwUr-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bad211e-7105-4bfa-d0e1-373b60114475"
      },
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "quantize_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp2v__ltcn/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp2v__ltcn/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbEfckeezip4",
        "outputId": "01482765-d208-4326-eac4-a8ea9fd744ef"
      },
      "source": [
        "# Create the .tflite file\n",
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/caps_compressedB.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(mm)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf79v0cle/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf79v0cle/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS8fBOSg0_tO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t76LFjYd1Ctg"
      },
      "source": [
        "# caps_basemodel.B = 27M\n",
        "# caps_compressedB (no pruning)  = 11MB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWt9eE4hpaDy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjFKKcxw2gVo",
        "outputId": "fca24614-4a15-4ef2-82c9-1863e29bab82"
      },
      "source": [
        "#checking for model 4\n",
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27M\t/content/drive/MyDrive/MnistResults/best_weights4.hdf5\n",
            "11M\t/content/drive/MyDrive/MnistResults/compressed1.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights1.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights2.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights3.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights4.hdf5\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs2.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs3.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs4.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file1.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file2.h5\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode2.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode3.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel4.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1c026FiF3Hb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTzAAQHpL51b"
      },
      "source": [
        "# Use this tutorial for pruning\n",
        "quantization has already been done earlier\n",
        "\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUQtzXdBqPh6"
      },
      "source": [
        "# PRUNING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAVk1mxJqykT"
      },
      "source": [
        "# pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vlXMULqdp9vq",
        "outputId": "0ecab66d-4a3a-465d-f4b2-139be2e686cf"
      },
      "source": [
        "comparison_metric.name\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my_accuracy_8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O46U_dp_w7jX",
        "outputId": "879dab72-7269-43c8-e037-139b855e19cd"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# TODO: Remove this line\n",
        "#m.load_weights('/content/drive/MyDrive/MnistResults/best_weights4.hdf5')\n",
        "\n",
        "# print(\"ORIGINAL MODEL\")\n",
        "# print(mm.summary())\n",
        "# print('-' * 80)\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = x_train.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "mm = m\n",
        "mm.load_weights('/content/drive/MyDrive/MnistResults/best_weights4.hdf5')\n",
        "\n",
        "# Define model for pruning.\n",
        "# pruning_params = {\n",
        "#       'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "#                                                                final_sparsity=0.80,\n",
        "#                                                                begin_step=0,\n",
        "#                                                                end_step=end_step)\n",
        "# }\n",
        "\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.20,\n",
        "                                                              final_sparsity=0.65,\n",
        "                                                              begin_step=0,\n",
        "                                                              end_step=end_step)\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "model_for_pruning = prune_low_magnitude(mm, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "######################################################################\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "# Helper function uses `prune_low_magnitude` to make only the \n",
        "# Dense layers train with pruning.\n",
        "def apply_pruning_to_layers(layer):\n",
        "  #print(\"called\")\n",
        "  if isinstance(layer, MyDigitCapsLayer):\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"conv_layer_2\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"routing1\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"conv_layer_1\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "\n",
        "\n",
        "  print(f\"Layer {layer} {layer.name} unchanged\")\n",
        "  return layer\n",
        "\n",
        "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_layers` \n",
        "# to the layers of the model.\n",
        "model_for_pruning = tf.keras.models.clone_model(\n",
        "    mm,\n",
        "    clone_function=apply_pruning_to_layers,\n",
        ")\n",
        "#print(model_for_pruning.summary())\n",
        "\n",
        "\"\"\"\n",
        "model_for_pruning = K.models.Sequential(\\\n",
        "              [\\\n",
        "                model_for_pruning,\\\n",
        "                MyReshapeLayer(),\\\n",
        "              ]\\\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model_for_pruning.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "#model_for_pruning.compile(optimizer='adam', loss=MarginLoss())\n",
        "\n",
        "model_for_pruning.fit(x_train, y_train,\n",
        "                  batch_size=32, epochs=2, validation_split=validation_split,\n",
        "                  callbacks=callbacks, verbose = 1)\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, y_test, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer <keras.layers.convolutional.Conv2D object at 0x7f2171de4a50> conv_layer_1 slated for pruning\n",
            "Layer <keras.layers.convolutional.Conv2D object at 0x7f2171de4b50> conv_layer_2 slated for pruning\n",
            "Layer <keras.layers.core.Reshape object at 0x7f2171de49d0> reshape_layer_1 unchanged\n",
            "Layer <__main__.SquashLayer object at 0x7f21607677d0> caps1_output_layer unchanged\n",
            "Layer <__main__.MyDigitCapsLayer object at 0x7f2171eb3190> caps2_predicted slated for pruning\n",
            "Layer <__main__.RoutingByAgreement object at 0x7f21606d6c10> routing1 slated for pruning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "   6/1688 [..............................] - ETA: 5:56 - loss: 2.2786 - my_accuracy_2: 1.0000WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0686s vs `on_train_batch_end` time: 0.0773s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0686s vs `on_train_batch_end` time: 0.0773s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1688/1688 [==============================] - 144s 84ms/step - loss: 2.2778 - my_accuracy_2: 0.9484 - val_loss: 2.2755 - val_my_accuracy_2: 0.9708\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 142s 84ms/step - loss: 2.2778 - my_accuracy_2: 0.9560 - val_loss: 2.2754 - val_my_accuracy_2: 0.9793\n",
            "313/313 [==============================] - 11s 34ms/step - loss: 2.2765 - my_accuracy_2: 0.9763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPhCXWsoyk-J",
        "outputId": "d7ad0792-8a08-4729-d2e8-cf5cd8fe2baa"
      },
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 10s 33ms/step - loss: 2.2765 - my_accuracy_2: 0.9763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Si86o_d47KL",
        "outputId": "afb20033-da6c-42d3-e7cc-a7668c1403d6"
      },
      "source": [
        "print(\"prunned model evaluation time:\" + str((10/x_test.shape[0])*1000) + \"ms, for 28X28 image\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prunned model evaluation time:1.0ms, for 28X28 image\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdWbPGn-5cfy"
      },
      "source": [
        "## prunned model evaluation time:1.0ms, for 28X28 image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ8YDxRv9r8N"
      },
      "source": [
        "\n",
        "#explanation for increase in parameters after pruning\n",
        "\n",
        "https://www.dlology.com/blog/how-to-compress-your-keras-model-x5-smaller-with-tensorflow-model-optimization/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bId0Lw6sx0VW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD9aZWk2D5Bh",
        "outputId": "b767ccab-2928-4254-efcb-b361c71aaab4"
      },
      "source": [
        "# _, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "#    x_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 11s 34ms/step - loss: 2.2769 - my_accuracy_72: 0.5371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU7MOJdAOcud",
        "outputId": "eddeabee-fb42-489b-9e30-18e9c73790b1"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()\n",
        "pruned_keras_file = \"/content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\"\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved pruned Keras model to: /content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-LY0Hfr59mV"
      },
      "source": [
        "# size of pruned model = 27M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5yFQWwf-ae-",
        "outputId": "b74c3210-0e96-4ff8-d142-7552a63da20a"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = model_for_export\n",
        "import numpy as np\n",
        "\n",
        "for i, w in enumerate(model.get_weights()):\n",
        "  print(\"{} -- Total:{}, Zeros: {:.2f}%\".format(model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv_layer_1/kernel:0 -- Total:20736, Zeros: 64.99%\n",
            "conv_layer_1/bias:0 -- Total:256, Zeros: 0.00%\n",
            "conv_layer_2/kernel:0 -- Total:5308416, Zeros: 64.99%\n",
            "conv_layer_2/bias:0 -- Total:256, Zeros: 0.00%\n",
            "caps2_predicted/kernel:0 -- Total:1474560, Zeros: 64.99%\n",
            "routing1/raw_weights:0 -- Total:11520, Zeros: 64.99%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgDI-tWRAR6j",
        "outputId": "3b9806dd-da02-425e-aad9-d74271e3166a"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4ppdoc68/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp4ppdoc68/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "Btf00j9rAWGg",
        "outputId": "1a06b4c1-ca5f-4c9f-e9c9-c7e22f7b8135"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5912c478d548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressed_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflite_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'evaluate'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcGSgMOPpQLp"
      },
      "source": [
        "tyoes of post training quantization\n",
        "https://www.tensorflow.org/lite/performance/post_training_quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2cMII_O-ctp"
      },
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FnrEOrH4vC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c580bfa-a9ed-449d-fcbc-7ecbb7db17a5"
      },
      "source": [
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/caps_P&Q_1.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf4v5i690/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpf4v5i690/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7zi0rYV-538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd382106-82e2-4e1a-ac7e-15bed15bbc25"
      },
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(\"/content/drive/MyDrive/MnistResults/best_weights4.hdf5\")))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(tflite_model_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 25639770.00 bytes\n",
            "Size of gzipped pruned Keras model: 11977511.00 bytes\n",
            "Size of gzipped pruned TFlite model: 4750440.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Foyx1OS-Z12"
      },
      "source": [
        "def get_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  #import zipfile\n",
        "\n",
        "  _, model_file = tempfile.mkstemp('.tflite')\n",
        "  with(model_file, 'w') as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kut_pL4RBDBJ",
        "outputId": "e0d8e3a9-1cc4-4797-969c-6ffbeed41fcc"
      },
      "source": [
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/caps_P&Q_1_noZip.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxygeqd7y/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxygeqd7y/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1smem26RCser",
        "outputId": "21d7cc76-684b-4475-aee4-fdd1e08f46d4"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27M\t/content/drive/MyDrive/MnistResults/best_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/best_weights5.hdf5\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\n",
            "3.0K\t/content/drive/MyDrive/MnistResults/caps48X48mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/caps_basemodelA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelA.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelB.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_best_weightsA.hdf5\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedA.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedB.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_latest_weightsA.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps_mylogsA.csv\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1_noZip.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_save_basemodelA.tf\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_best_weights4.hdf5\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_latest_weights4.hdf5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/cnn_mylogs4.csv\n",
            "367K\t/content/drive/MyDrive/MnistResults/cnn_save_basemodel4.tf\n",
            "11M\t/content/drive/MyDrive/MnistResults/compressed1.tflite\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_best_weights1.hdf5\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_latest_weights1.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/dense_mylogs1.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights1.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights2.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights3.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights5.hdf5\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs2.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs3.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs4.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file1.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file2.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_30.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_60.h5\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode2.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode3.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel4.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel5.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg32X32basemodel.h5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights4.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights4.hdf5\n",
            "2.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/vggfine_mylogs2.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs4.csv\n",
            "158M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel4.tf\n",
            "59M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights2.hdf5\n",
            "0\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1 (1).csv\n",
            "1.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs2.csv\n",
            "231M\t/content/drive/MyDrive/MnistResults/vggtrain_pruned_model2.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vggtrain_prunedStripped_basemodel2.tf\n",
            "171M\t/content/drive/MyDrive/MnistResults/vggtrain_save_basemodel2.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2wmXZakEDyD",
        "outputId": "93d3bc90-6138-4eeb-89fd-caccfd2673fe"
      },
      "source": [
        "keras_file = \"/content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\"\n",
        "tf.keras.models.save_model(m, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V72rmunKFD3c",
        "outputId": "483b21b8-a8a9-46cd-de55-a0d195e7d654"
      },
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(tflite_model_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 11977519.00 bytes\n",
            "Size of gzipped pruned Keras model: 11977511.00 bytes\n",
            "Size of gzipped pruned TFlite model: 4750452.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxYmi2X-Fyux"
      },
      "source": [
        "## =================================================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp5t2drKI9Gg"
      },
      "source": [
        "# VGG compression 32X32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSwB0iiEJFdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa379418-463b-48f6-d38b-e83e66c6f2b3"
      },
      "source": [
        "# Load MNIST dataset\n",
        "import keras\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "train_X =[]\n",
        "\n",
        "for i in range(train_images.shape[0]):\n",
        "  train_pad = np.pad(train_images[i], pad_width=2 , mode= 'edge')\n",
        "  #print(train_pad)\n",
        "  train_X.append(train_pad)\n",
        "#PADDING test images\n",
        "test_X = []\n",
        "for i in range(test_images.shape[0]):\n",
        "  test_pad = np.pad(test_images[i], pad_width=2 , mode= 'edge')\n",
        "  #print(train_pad)\n",
        "  test_X.append(test_pad)\n",
        "train_X = np.array(train_X)\n",
        "test_X = np.array(test_X)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkQoaLEfJ1B-",
        "outputId": "9ff8e520-6ffe-46fa-d78a-a96dc0f35717"
      },
      "source": [
        "train_X=np.dstack([train_X] * 3)\n",
        "test_X=np.dstack([test_X]*3)\n",
        "train_X.shape,test_X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 32, 96), (10000, 32, 96))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDg0H7tDKFsL",
        "outputId": "101c8afc-7ca0-4e20-95c9-b3c287a6a1ec"
      },
      "source": [
        "train_X = train_X.reshape(-1, 32,32,3)\n",
        "test_X= test_X.reshape (-1,32,32,3)\n",
        "\n",
        "train_X = train_X.astype('float32')\n",
        "test_X = test_X.astype('float32')\n",
        "train_X.shape,test_X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTDF3q1yKLr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e862e060-d6e6-407b-f5d0-39802f86b2a1"
      },
      "source": [
        "vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vggModel.trainable = True\n",
        "model2 = keras.Sequential([\n",
        "  vggModel,\n",
        "  keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  keras.layers.Dense(256, activation='relu'),\n",
        "  keras.layers.Dense(50, activation = 'relu'),\n",
        "  keras.layers.Dense(10, activation = 'softmax')\n",
        "])\n",
        "# Train the digit classification model\n",
        "model2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# history = model2.fit(\n",
        "#   train_X,\n",
        "#   train_labels,\n",
        "#   epochs= 50,\n",
        "#   validation_split=0.1, callbacks = callback_list\n",
        "# )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT4kWs_Ffz2L",
        "outputId": "1fcb29be-06ef-434f-c05e-5f363257fc84"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                12850     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 14,859,376\n",
            "Trainable params: 14,859,376\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjyqPpN6Kz1t"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "#comparison_metric = MyAccuracy()\n",
        "#checkpoint_filepath = \"/content/drive/MyDrive/Weights/weights-improvement-{epoch:02d}-{val_my_accuracy:.2f}.hdf5\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = \"/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\",\n",
        "        save_weights_only=True,\n",
        "        monitor=\"val_accuracy\",\n",
        "        #monitor=\"val_my_accuracy_19\",\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "model_checkpoint_callback2 = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = \"/content/drive/MyDrive/MnistResults/cnn_latest_weights4.hdf5\",\n",
        "        save_weights_only=True,\n",
        "        monitor=\"val_accuracy\",\n",
        "        mode='max',\n",
        "        save_best_only=False)\n",
        "log_csv = CSVLogger(\"/content/drive/MyDrive/MnistResults/cnn_mylogs4.csv\", separator = \",\", append = False)\n",
        "callback_list = [model_checkpoint_callback, model_checkpoint_callback2, log_csv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFQZXd8KKrgO",
        "outputId": "f21a0b52-ef77-428e-e0df-6b479385ff5b"
      },
      "source": [
        "import time\n",
        "class TimeHistory(K.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "time_callback = TimeHistory()\n",
        "history = model2.fit( train_X,train_labels, epochs= 10,validation_split=0.1, callbacks = [time_callback])\n",
        "times = time_callback.times\n",
        "print(times)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   5/1688 [..............................] - ETA: 46s - loss: 2.7175 - accuracy: 0.0938WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0119s vs `on_train_batch_end` time: 0.0132s). Check your callbacks.\n",
            "1688/1688 [==============================] - 65s 28ms/step - loss: 0.7341 - accuracy: 0.7338 - val_loss: 0.1280 - val_accuracy: 0.9728\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 48s 28ms/step - loss: 0.1629 - accuracy: 0.9624 - val_loss: 0.0772 - val_accuracy: 0.9815\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 48s 28ms/step - loss: 0.1307 - accuracy: 0.9711 - val_loss: 0.0643 - val_accuracy: 0.9842\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 49s 29ms/step - loss: 0.1000 - accuracy: 0.9786 - val_loss: 0.0525 - val_accuracy: 0.9880\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.0828 - accuracy: 0.9824 - val_loss: 0.0742 - val_accuracy: 0.9843\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.1040 - accuracy: 0.9791 - val_loss: 0.0500 - val_accuracy: 0.9887\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.0717 - accuracy: 0.9852 - val_loss: 0.0726 - val_accuracy: 0.9848\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.0836 - accuracy: 0.9835 - val_loss: 0.0564 - val_accuracy: 0.9883\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.0675 - accuracy: 0.9854 - val_loss: 0.0534 - val_accuracy: 0.9913\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 48s 29ms/step - loss: 0.0846 - accuracy: 0.9853 - val_loss: 0.0720 - val_accuracy: 0.9842\n",
            "[65.0309910774231, 47.71386194229126, 47.914467096328735, 49.366514444351196, 48.40263247489929, 48.482271671295166, 48.43564486503601, 48.367270946502686, 48.31699013710022, 48.35564160346985]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91KboyrEfFoV",
        "outputId": "b1d9df3d-ea0a-4832-c1be-3678d31b281e"
      },
      "source": [
        "average_vgg = sum(times)/len(times)\n",
        "print(average_vgg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.03862862586975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFMRTZHxS250",
        "outputId": "aecc8645-a2d3-4816-d699-0bfb177e03e0"
      },
      "source": [
        "print(\"training time for vgg: \" + str((average_vgg/54000)*1000) + \" ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training time for vgg: 0.9266412708494397ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzGMIygfTPDV",
        "outputId": "11f4f6b9-f819-478d-c774-d32af2fb2a68"
      },
      "source": [
        "print(average_vgg/(54000/32)*1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29.65252066718207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwbND9jRN6N"
      },
      "source": [
        "model2.load_weights(\"/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2baEMjH4SFPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KVKNDHdR0Dx",
        "outputId": "a975bc67-5cc8-4068-cba4-755fd5e54c1f"
      },
      "source": [
        "h = model2.evaluate(test_X, test_labels, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 17s 8ms/step - loss: 0.0408 - accuracy: 0.9903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4VejjcISWcq",
        "outputId": "37194397-31cd-4dcf-d3ed-6507d6552d31"
      },
      "source": [
        "print(\"evaluation time for vgg: \" + str((3/test_X.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation time for vgg: 0.3ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_BpTyEwkQGh"
      },
      "source": [
        "## evaluation time for vgg: 0.3ms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6DGMSpEjN_d"
      },
      "source": [
        "model2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model2.load_weights(\"/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9RnrLqYikUW"
      },
      "source": [
        "# y_pred = model2.predict(test_X)\n",
        "# #print(y_pred)\n",
        "\n",
        "# import sklearn\n",
        "# from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
        "# #print(confusion_matrix(test_labels, y_pred))\n",
        "# print(f\"accuracy = {accuracy_score(test_labels, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjoxXMr1UGfB",
        "outputId": "ce835ec3-8995-4b68-f43b-dcc5194e0e5f"
      },
      "source": [
        "vgg_keras_file = \"/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.h5\"\n",
        "tf.keras.models.save_model(model2, vgg_keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', vgg_keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTpMYin8geuu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce-Zxht3WPuR"
      },
      "source": [
        "## size of VGG .h5 base model = 57M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM7vD8YRW21B",
        "outputId": "2ee90092-357d-4a94-b7bd-e38b5abb8fc4"
      },
      "source": [
        "vgg_keras_file_tf = \"/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf\"\n",
        "tf.keras.models.save_model(model2, vgg_keras_file_tf, include_optimizer=False)\n",
        "print('Saved baseline model to:', vgg_keras_file_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZffgKBDbXPsB"
      },
      "source": [
        "# VGG .tf base model size = 58M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCe2oUbJf0WY"
      },
      "source": [
        "model2.load_weights(\"/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEm7VUQkgBuI",
        "outputId": "457f03ca-c914-426b-953f-81bec8796841"
      },
      "source": [
        "vgg_keras_file_tf = \"/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\"\n",
        "tf.keras.models.save_model(model2, vgg_keras_file_tf, include_optimizer=False)\n",
        "print('Saved baseline model to:', vgg_keras_file_tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved baseline model to: /content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYuHQM8gVK7_"
      },
      "source": [
        "modelVgg = model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV2xG_78VFE7"
      },
      "source": [
        "# compression without pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw1g290bYgVz",
        "outputId": "776282e0-c18c-4f89-a224-e6e34d548d9d"
      },
      "source": [
        "# Create the .tflite file\n",
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/vgg_compressed32X32_1.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(modelVgg)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpjs6a2a91/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6xI6NZmY9Oz"
      },
      "source": [
        "# VGG .tflite Compressed without Pruning  = 15M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRjg53pZsP8"
      },
      "source": [
        "# Pruning VGG\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn7VFq3VfHr9",
        "outputId": "3e7ac3d5-c594-4407-d44d-07f533621504"
      },
      "source": [
        "import tempfile\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "finalSparsity = 0.05\n",
        "list_accuracy0 = []\n",
        "while finalSparsity <= 0.95:\n",
        "  model2.load_weights('/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5')\n",
        "  pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                                final_sparsity=finalSparsity,\n",
        "                                                                begin_step=0,\n",
        "                                                                end_step=end_step)\n",
        "  }\n",
        "  \n",
        "  # Define model for pruning.\n",
        "  model_for_pruning = prune_low_magnitude(model2, **pruning_params)\n",
        "\n",
        "  # `prune_low_magnitude` requires a recompile.\n",
        "  model_for_pruning.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  #model_for_pruning.summary()\n",
        "  logdir = tempfile.mkdtemp()\n",
        "\n",
        "  callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "  ]\n",
        "\n",
        "  model_for_pruning.fit(train_X, train_labels,\n",
        "                    batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                    callbacks=callbacks)\n",
        "  _, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_X, test_labels, verbose=0)\n",
        "  print(f'Pruned test accuracy at final_sparsity: at {finalSparsity} is {model_for_pruning_accuracy}')\n",
        "  list_accuracy0.append(model_for_pruning_accuracy)\n",
        "  finalSparsity += 0.05\n",
        "print(list_accuracy0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:24 - loss: 4.0409 - accuracy: 0.2781WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0294s vs `on_train_batch_begin` time: 0.0322s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0294s vs `on_train_batch_end` time: 0.0856s). Check your callbacks.\n",
            "422/422 [==============================] - 31s 51ms/step - loss: 0.3138 - accuracy: 0.9014 - val_loss: 0.0511 - val_accuracy: 0.9903\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0311 - accuracy: 0.9931 - val_loss: 0.0464 - val_accuracy: 0.9912\n",
            "Pruned test accuracy at final_sparsity: at 0.05 is 0.9915000200271606\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:39 - loss: 3.6157 - accuracy: 0.2500WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0307s vs `on_train_batch_begin` time: 0.0342s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0307s vs `on_train_batch_end` time: 0.1394s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3316 - accuracy: 0.8883 - val_loss: 0.0475 - val_accuracy: 0.9917\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0292 - accuracy: 0.9939 - val_loss: 0.0528 - val_accuracy: 0.9917\n",
            "Pruned test accuracy at final_sparsity: at 0.1 is 0.9911999702453613\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:56 - loss: 3.8489 - accuracy: 0.2562WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0300s vs `on_train_batch_begin` time: 0.0334s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0300s vs `on_train_batch_end` time: 0.1356s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3252 - accuracy: 0.9034 - val_loss: 0.0714 - val_accuracy: 0.9862\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0375 - accuracy: 0.9923 - val_loss: 0.0470 - val_accuracy: 0.9927\n",
            "Pruned test accuracy at final_sparsity: at 0.15000000000000002 is 0.9925000071525574\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:40 - loss: 3.6225 - accuracy: 0.2552WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_begin` time: 0.0347s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_end` time: 0.1410s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3532 - accuracy: 0.8891 - val_loss: 0.0671 - val_accuracy: 0.9903\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0305 - accuracy: 0.9936 - val_loss: 0.0511 - val_accuracy: 0.9923\n",
            "Pruned test accuracy at final_sparsity: at 0.2 is 0.9921000003814697\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:02 - loss: 3.5509 - accuracy: 0.3047WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_begin` time: 0.0339s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_end` time: 0.1438s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3232 - accuracy: 0.8998 - val_loss: 0.0496 - val_accuracy: 0.9910\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0360 - accuracy: 0.9930 - val_loss: 0.0620 - val_accuracy: 0.9872\n",
            "Pruned test accuracy at final_sparsity: at 0.25 is 0.9837999939918518\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:15 - loss: 4.0437 - accuracy: 0.2766WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0298s vs `on_train_batch_begin` time: 0.0331s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0298s vs `on_train_batch_end` time: 0.1665s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.2915 - accuracy: 0.9087 - val_loss: 0.0542 - val_accuracy: 0.9915\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0662 - accuracy: 0.9853 - val_loss: 0.0435 - val_accuracy: 0.9923\n",
            "Pruned test accuracy at final_sparsity: at 0.3 is 0.9908999800682068\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 2:16 - loss: 3.3354 - accuracy: 0.2669WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_begin` time: 0.0354s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_end` time: 0.2134s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 52ms/step - loss: 0.3882 - accuracy: 0.8804 - val_loss: 0.1094 - val_accuracy: 0.9885\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0419 - accuracy: 0.9913 - val_loss: 0.0574 - val_accuracy: 0.9907\n",
            "Pruned test accuracy at final_sparsity: at 0.35 is 0.988099992275238\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:59 - loss: 4.0313 - accuracy: 0.2641WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0338s vs `on_train_batch_begin` time: 0.0348s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0338s vs `on_train_batch_end` time: 0.1367s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3564 - accuracy: 0.8822 - val_loss: 0.0862 - val_accuracy: 0.9892\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0613 - accuracy: 0.9925 - val_loss: 0.0803 - val_accuracy: 0.9905\n",
            "Pruned test accuracy at final_sparsity: at 0.39999999999999997 is 0.9901999831199646\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:34 - loss: 3.7941 - accuracy: 0.3086WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_begin` time: 0.0335s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.1313s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3551 - accuracy: 0.8832 - val_loss: 0.1062 - val_accuracy: 0.9913\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.0897 - accuracy: 0.9888 - val_loss: 0.0478 - val_accuracy: 0.9922\n",
            "Pruned test accuracy at final_sparsity: at 0.44999999999999996 is 0.9909999966621399\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:39 - loss: 3.7773 - accuracy: 0.2617WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_begin` time: 0.0353s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_end` time: 0.1386s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.3409 - accuracy: 0.8851 - val_loss: 0.0441 - val_accuracy: 0.9890\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0693 - accuracy: 0.9851 - val_loss: 0.0377 - val_accuracy: 0.9910\n",
            "Pruned test accuracy at final_sparsity: at 0.49999999999999994 is 0.9907000064849854\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:58 - loss: 3.7991 - accuracy: 0.2812WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0328s vs `on_train_batch_begin` time: 0.0329s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0328s vs `on_train_batch_end` time: 0.1380s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.4147 - accuracy: 0.8537 - val_loss: 0.2503 - val_accuracy: 0.8958\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.1929 - accuracy: 0.9336 - val_loss: 0.0570 - val_accuracy: 0.9910\n",
            "Pruned test accuracy at final_sparsity: at 0.5499999999999999 is 0.9907000064849854\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:36 - loss: 3.6780 - accuracy: 0.2812WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_begin` time: 0.0353s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_end` time: 0.1329s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.4473 - accuracy: 0.8513 - val_loss: 0.2172 - val_accuracy: 0.9293\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.4569 - accuracy: 0.8644 - val_loss: 0.1915 - val_accuracy: 0.9855\n",
            "Pruned test accuracy at final_sparsity: at 0.6 is 0.9853000044822693\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:53 - loss: 3.6219 - accuracy: 0.2786WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0316s vs `on_train_batch_begin` time: 0.0345s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0316s vs `on_train_batch_end` time: 0.1676s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.4940 - accuracy: 0.8426 - val_loss: 0.7593 - val_accuracy: 0.6703\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.6067 - accuracy: 0.7749 - val_loss: 0.4467 - val_accuracy: 0.7985\n",
            "Pruned test accuracy at final_sparsity: at 0.65 is 0.8016999959945679\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:41 - loss: 4.3216 - accuracy: 0.2531WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0305s vs `on_train_batch_begin` time: 0.0366s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0305s vs `on_train_batch_end` time: 0.2050s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.6609 - accuracy: 0.7915 - val_loss: 1.2027 - val_accuracy: 0.5825\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 1.0006 - accuracy: 0.6045 - val_loss: 0.1348 - val_accuracy: 0.9867\n",
            "Pruned test accuracy at final_sparsity: at 0.7000000000000001 is 0.9854000210762024\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:00 - loss: 3.9488 - accuracy: 0.2672WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0323s vs `on_train_batch_begin` time: 0.0361s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0323s vs `on_train_batch_end` time: 0.1377s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 52ms/step - loss: 0.4676 - accuracy: 0.8456 - val_loss: 0.4361 - val_accuracy: 0.7058\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.1591 - accuracy: 0.9434 - val_loss: 0.0505 - val_accuracy: 0.9887\n",
            "Pruned test accuracy at final_sparsity: at 0.7500000000000001 is 0.9858999848365784\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:39 - loss: 3.5497 - accuracy: 0.2435WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0300s vs `on_train_batch_begin` time: 0.0346s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0300s vs `on_train_batch_end` time: 0.1397s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.8038 - accuracy: 0.7137 - val_loss: 0.9323 - val_accuracy: 0.5830\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.3889 - accuracy: 0.8263 - val_loss: 0.1915 - val_accuracy: 0.8940\n",
            "Pruned test accuracy at final_sparsity: at 0.8000000000000002 is 0.8931999802589417\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:01 - loss: 4.6646 - accuracy: 0.2453WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0315s vs `on_train_batch_begin` time: 0.0338s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0315s vs `on_train_batch_end` time: 0.1414s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.8452 - accuracy: 0.7081 - val_loss: 1.2521 - val_accuracy: 0.4070\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.3286 - accuracy: 0.8441 - val_loss: 0.0523 - val_accuracy: 0.9895\n",
            "Pruned test accuracy at final_sparsity: at 0.8500000000000002 is 0.9901999831199646\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:38 - loss: 3.5846 - accuracy: 0.2812WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0290s vs `on_train_batch_begin` time: 0.0345s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0290s vs `on_train_batch_end` time: 0.1395s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.9614 - accuracy: 0.6704 - val_loss: 1.2498 - val_accuracy: 0.4980\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.6625 - accuracy: 0.7016 - val_loss: 0.2319 - val_accuracy: 0.8795\n",
            "Pruned test accuracy at final_sparsity: at 0.9000000000000002 is 0.8820000290870667\n",
            "[0.9915000200271606, 0.9911999702453613, 0.9925000071525574, 0.9921000003814697, 0.9837999939918518, 0.9908999800682068, 0.988099992275238, 0.9901999831199646, 0.9909999966621399, 0.9907000064849854, 0.9907000064849854, 0.9853000044822693, 0.8016999959945679, 0.9854000210762024, 0.9858999848365784, 0.8931999802589417, 0.9901999831199646, 0.8820000290870667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ray_YkzU6KHL",
        "outputId": "689bd1cf-f593-4838-827e-cde26969b62b"
      },
      "source": [
        "import tempfile\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "finalSparsity = 0.05\n",
        "list_accuracy0 = []\n",
        "while finalSparsity <= 0.95:\n",
        "  model2.load_weights('/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5')\n",
        "  pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                                final_sparsity=finalSparsity,\n",
        "                                                                begin_step=0,\n",
        "                                                                end_step=end_step)\n",
        "  }\n",
        "  \n",
        "  # Define model for pruning.\n",
        "  model_for_pruning = prune_low_magnitude(model2, **pruning_params)\n",
        "\n",
        "  # `prune_low_magnitude` requires a recompile.\n",
        "  model_for_pruning.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  #model_for_pruning.summary()\n",
        "  logdir = tempfile.mkdtemp()\n",
        "\n",
        "  callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "  ]\n",
        "\n",
        "  model_for_pruning.fit(train_X, train_labels,\n",
        "                    batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                    callbacks=callbacks)\n",
        "  _, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_X, test_labels, verbose=0)\n",
        "  print(f'Pruned test accuracy at final_sparsity: at {finalSparsity} is {model_for_pruning_accuracy}')\n",
        "  list_accuracy0.append(model_for_pruning_accuracy)\n",
        "  finalSparsity += 0.05\n",
        "print(list_accuracy0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:59 - loss: 3.5924 - accuracy: 0.2781WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0295s vs `on_train_batch_begin` time: 0.0354s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0295s vs `on_train_batch_end` time: 0.1391s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3284 - accuracy: 0.8901 - val_loss: 0.1037 - val_accuracy: 0.9843\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0569 - accuracy: 0.9893 - val_loss: 0.0549 - val_accuracy: 0.9923\n",
            "Pruned test accuracy at final_sparsity: at 0.05 is 0.9901999831199646\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:43 - loss: 3.6514 - accuracy: 0.2643WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0296s vs `on_train_batch_begin` time: 0.0351s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0296s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.2917 - accuracy: 0.9062 - val_loss: 0.1172 - val_accuracy: 0.9913\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0508 - accuracy: 0.9896 - val_loss: 0.0419 - val_accuracy: 0.9913\n",
            "Pruned test accuracy at final_sparsity: at 0.1 is 0.9904999732971191\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:43 - loss: 3.0504 - accuracy: 0.2865WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0337s vs `on_train_batch_begin` time: 0.0343s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0337s vs `on_train_batch_end` time: 0.1445s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3092 - accuracy: 0.9064 - val_loss: 0.0551 - val_accuracy: 0.9898\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0319 - accuracy: 0.9934 - val_loss: 0.0498 - val_accuracy: 0.9927\n",
            "Pruned test accuracy at final_sparsity: at 0.15000000000000002 is 0.992900013923645\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:58 - loss: 3.6962 - accuracy: 0.2547WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0291s vs `on_train_batch_begin` time: 0.0335s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0291s vs `on_train_batch_end` time: 0.1398s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3511 - accuracy: 0.8824 - val_loss: 0.0599 - val_accuracy: 0.9917\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0375 - accuracy: 0.9920 - val_loss: 0.0529 - val_accuracy: 0.9927\n",
            "Pruned test accuracy at final_sparsity: at 0.2 is 0.9918000102043152\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:38 - loss: 4.1755 - accuracy: 0.2487WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0313s vs `on_train_batch_begin` time: 0.0330s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0313s vs `on_train_batch_end` time: 0.1385s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.3417 - accuracy: 0.8884 - val_loss: 0.0578 - val_accuracy: 0.9893\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0381 - accuracy: 0.9923 - val_loss: 0.0574 - val_accuracy: 0.9922\n",
            "Pruned test accuracy at final_sparsity: at 0.25 is 0.9923999905586243\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 2:05 - loss: 3.9715 - accuracy: 0.2766WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0293s vs `on_train_batch_begin` time: 0.0366s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0293s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.3597 - accuracy: 0.8834 - val_loss: 0.0975 - val_accuracy: 0.9832\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0413 - accuracy: 0.9914 - val_loss: 0.0426 - val_accuracy: 0.9940\n",
            "Pruned test accuracy at final_sparsity: at 0.3 is 0.9921000003814697\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:54 - loss: 4.1803 - accuracy: 0.2625WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_begin` time: 0.0335s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_end` time: 0.1326s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3434 - accuracy: 0.8888 - val_loss: 0.0478 - val_accuracy: 0.9900\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0932 - accuracy: 0.9804 - val_loss: 0.0816 - val_accuracy: 0.9898\n",
            "Pruned test accuracy at final_sparsity: at 0.35 is 0.9878000020980835\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:40 - loss: 3.5984 - accuracy: 0.2891WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_begin` time: 0.0362s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0304s vs `on_train_batch_end` time: 0.1398s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.4105 - accuracy: 0.8663 - val_loss: 0.1296 - val_accuracy: 0.9822\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0457 - accuracy: 0.9919 - val_loss: 0.0444 - val_accuracy: 0.9908\n",
            "Pruned test accuracy at final_sparsity: at 0.39999999999999997 is 0.9884999990463257\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:44 - loss: 3.5938 - accuracy: 0.2773WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0307s vs `on_train_batch_begin` time: 0.0353s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0307s vs `on_train_batch_end` time: 0.1497s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 52ms/step - loss: 0.3984 - accuracy: 0.8641 - val_loss: 0.2165 - val_accuracy: 0.8962\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0587 - accuracy: 0.9780 - val_loss: 0.0393 - val_accuracy: 0.9918\n",
            "Pruned test accuracy at final_sparsity: at 0.44999999999999996 is 0.9921000003814697\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:56 - loss: 4.3397 - accuracy: 0.2984WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0296s vs `on_train_batch_begin` time: 0.0333s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0296s vs `on_train_batch_end` time: 0.1368s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 50ms/step - loss: 0.3628 - accuracy: 0.8788 - val_loss: 0.1122 - val_accuracy: 0.9905\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0463 - accuracy: 0.9908 - val_loss: 0.0318 - val_accuracy: 0.9935\n",
            "Pruned test accuracy at final_sparsity: at 0.49999999999999994 is 0.9911999702453613\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:57 - loss: 3.8625 - accuracy: 0.2878WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0290s vs `on_train_batch_begin` time: 0.0356s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0290s vs `on_train_batch_end` time: 0.1752s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.3744 - accuracy: 0.8803 - val_loss: 0.0585 - val_accuracy: 0.9890\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.0802 - accuracy: 0.9710 - val_loss: 0.0942 - val_accuracy: 0.9915\n",
            "Pruned test accuracy at final_sparsity: at 0.5499999999999999 is 0.9925000071525574\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 2:15 - loss: 3.3820 - accuracy: 0.2891WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0292s vs `on_train_batch_begin` time: 0.0365s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0292s vs `on_train_batch_end` time: 0.2115s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 52ms/step - loss: 0.5700 - accuracy: 0.8183 - val_loss: 0.4900 - val_accuracy: 0.8028\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.4643 - accuracy: 0.8029 - val_loss: 0.2203 - val_accuracy: 0.8895\n",
            "Pruned test accuracy at final_sparsity: at 0.6 is 0.8859000205993652\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:39 - loss: 3.5935 - accuracy: 0.2826WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_begin` time: 0.0327s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_end` time: 0.1402s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 52ms/step - loss: 0.4328 - accuracy: 0.8566 - val_loss: 0.6439 - val_accuracy: 0.7918\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.3119 - accuracy: 0.8927 - val_loss: 0.0542 - val_accuracy: 0.9883\n",
            "Pruned test accuracy at final_sparsity: at 0.65 is 0.9871000051498413\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:57 - loss: 4.0676 - accuracy: 0.2578WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_begin` time: 0.0335s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0311s vs `on_train_batch_end` time: 0.1365s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.6188 - accuracy: 0.7840 - val_loss: 1.0698 - val_accuracy: 0.5867\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.5323 - accuracy: 0.7874 - val_loss: 0.0522 - val_accuracy: 0.9883\n",
            "Pruned test accuracy at final_sparsity: at 0.7000000000000001 is 0.9865999817848206\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:43 - loss: 3.4316 - accuracy: 0.2891WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0325s vs `on_train_batch_begin` time: 0.0349s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0325s vs `on_train_batch_end` time: 0.1467s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.6483 - accuracy: 0.7893 - val_loss: 0.9725 - val_accuracy: 0.5582\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 47ms/step - loss: 0.6875 - accuracy: 0.6774 - val_loss: 0.4425 - val_accuracy: 0.8852\n",
            "Pruned test accuracy at final_sparsity: at 0.7500000000000001 is 0.8871999979019165\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:43 - loss: 3.7913 - accuracy: 0.2513WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_begin` time: 0.0352s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0302s vs `on_train_batch_end` time: 0.1465s). Check your callbacks.\n",
            "422/422 [==============================] - 29s 51ms/step - loss: 0.7405 - accuracy: 0.7402 - val_loss: 0.9816 - val_accuracy: 0.5417\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.3065 - accuracy: 0.9079 - val_loss: 0.0732 - val_accuracy: 0.9862\n",
            "Pruned test accuracy at final_sparsity: at 0.8000000000000002 is 0.9840999841690063\n",
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 1:57 - loss: 4.1261 - accuracy: 0.2484WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_begin` time: 0.0340s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.1359s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.6578 - accuracy: 0.7684 - val_loss: 0.9075 - val_accuracy: 0.6357\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.3067 - accuracy: 0.8615 - val_loss: 0.0469 - val_accuracy: 0.9912\n",
            "Pruned test accuracy at final_sparsity: at 0.8500000000000002 is 0.9883999824523926\n",
            "Epoch 1/2\n",
            "  6/422 [..............................] - ETA: 1:38 - loss: 3.6670 - accuracy: 0.2695WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0306s vs `on_train_batch_begin` time: 0.0355s). Check your callbacks.\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0306s vs `on_train_batch_end` time: 0.1371s). Check your callbacks.\n",
            "422/422 [==============================] - 28s 51ms/step - loss: 0.9020 - accuracy: 0.6621 - val_loss: 0.9643 - val_accuracy: 0.6817\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.6848 - accuracy: 0.7414 - val_loss: 0.1714 - val_accuracy: 0.8803\n",
            "Pruned test accuracy at final_sparsity: at 0.9000000000000002 is 0.8840000033378601\n",
            "[0.9901999831199646, 0.9904999732971191, 0.992900013923645, 0.9918000102043152, 0.9923999905586243, 0.9921000003814697, 0.9878000020980835, 0.9884999990463257, 0.9921000003814697, 0.9911999702453613, 0.9925000071525574, 0.8859000205993652, 0.9871000051498413, 0.9865999817848206, 0.8871999979019165, 0.9840999841690063, 0.9883999824523926, 0.8840000033378601]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBB89b7c_Y2U",
        "outputId": "bb819cc6-2528-4bda-aac3-0035010ae4dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(list_accuracy0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9901999831199646, 0.9904999732971191, 0.992900013923645, 0.9918000102043152, 0.9923999905586243, 0.9921000003814697, 0.9878000020980835, 0.9884999990463257, 0.9921000003814697, 0.9911999702453613, 0.9925000071525574, 0.8859000205993652, 0.9871000051498413, 0.9865999817848206, 0.8871999979019165, 0.9840999841690063, 0.9883999824523926, 0.8840000033378601]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO83q4H0wDFh",
        "outputId": "64b9517f-ffc9-420f-cc78-5ae776e57cb2"
      },
      "source": [
        "print(list_accuracy0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9915000200271606, 0.9911999702453613, 0.9925000071525574, 0.9921000003814697, 0.9837999939918518, 0.9908999800682068, 0.988099992275238, 0.9901999831199646, 0.9909999966621399, 0.9907000064849854, 0.9907000064849854, 0.9853000044822693, 0.8016999959945679, 0.9854000210762024, 0.9858999848365784, 0.8931999802589417, 0.9901999831199646, 0.8820000290870667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnkQb1zEZrbZ",
        "outputId": "0679b6c9-3187-4ebf-c316-42a87d6416e3"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "\n",
        "\n",
        "model2.load_weights('/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5')\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.20,\n",
        "                                                              final_sparsity= 0.65 ,\n",
        "                                                              begin_step=0,\n",
        "                                                              end_step=end_step)\n",
        "}\n",
        "\n",
        "# Define model for pruning.\n",
        "model_for_pruning = prune_low_magnitude(model2, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model_for_pruning.summary()\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_X, train_labels,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "  test_X, test_labels, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  6/422 [..............................] - ETA: 1:37 - loss: 0.0456 - accuracy: 0.9909WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0352s vs `on_train_batch_begin` time: 0.0615s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0352s vs `on_train_batch_begin` time: 0.0615s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0352s vs `on_train_batch_end` time: 0.1057s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0352s vs `on_train_batch_end` time: 0.1057s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 37s 68ms/step - loss: 0.0420 - accuracy: 0.9916 - val_loss: 0.0418 - val_accuracy: 0.9920\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 26s 63ms/step - loss: 0.0297 - accuracy: 0.9933 - val_loss: 0.0411 - val_accuracy: 0.9938\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0329 - accuracy: 0.9932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg2F7Cbhb7QJ",
        "outputId": "363fbdde-ac92-4802-e19c-364755f8b466"
      },
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "  test_X, test_labels, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0329 - accuracy: 0.9932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqL_9i2lu-hv"
      },
      "source": [
        "# VGG pruned model accuracy = 99.1%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY1EOEiMb-U9",
        "outputId": "c223e859-c844-4e48-d2f3-c838fe5fd657"
      },
      "source": [
        "print(\"evaluation time for prunned vgg model: \" + str((3/test_X.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation time for prunned vgg model: 0.3ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h06y1L67cUEh"
      },
      "source": [
        "# evaluation time for prunned vgg model: 0.3ms, same as base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3b5NmFaY8l3",
        "outputId": "8d40db15-e633-4017-b67c-c97ea4f0e60e"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()\n",
        "pruned_keras_file = \"/content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\"\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 1, 1, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                12850     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 14,859,376\n",
            "Trainable params: 14,859,376\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved pruned Keras model to: /content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lb3mFObpy2a"
      },
      "source": [
        "## VGG Pruned 58M. same as original size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khp8EFNtp5di"
      },
      "source": [
        "# now Compression after pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HHXIF-Vp4jR",
        "outputId": "09254dd5-e1ee-4ccb-e55b-20b95ef686a3"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model_vgg = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model_vgg)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpcxsi305e/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpcxsi305e/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved pruned TFLite model to: /tmp/tmp1kjbqoik.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGrdqdTxgexs",
        "outputId": "2e8846fa-d6b3-47ca-fcb6-ccc88fd02cb8"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27M\t/content/drive/MyDrive/MnistResults/best_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/best_weights5.hdf5\n",
            "92M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_1.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_2.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_compressedB.tflite\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps32X32mylogs5.csv\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1_noZip.tflite\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1.tflite\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.h5\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.tf\n",
            "3.0K\t/content/drive/MyDrive/MnistResults/caps48X48mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/caps_basemodelA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelA.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelB.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_best_weightsA.hdf5\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedA.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedB.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_latest_weightsA.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps_mylogsA.csv\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1_noZip.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_save_basemodelA.tf\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_best_weights4.hdf5\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_latest_weights4.hdf5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/cnn_mylogs4.csv\n",
            "367K\t/content/drive/MyDrive/MnistResults/cnn_save_basemodel4.tf\n",
            "11M\t/content/drive/MyDrive/MnistResults/compressed1.tflite\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_best_weights1.hdf5\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_latest_weights1.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/dense_mylogs1.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights1.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights2.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights3.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights5.hdf5\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs2.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs3.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs4.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file1.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file2.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_30.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_60.h5\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode2.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode3.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel4.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel5.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg32X32basemodel.h5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg32X32_P&Q_1_noZip.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.h5\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_2.h5\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg_compressed32X32_1.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights4.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights4.hdf5\n",
            "2.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/vggfine_mylogs2.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs4.csv\n",
            "158M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel4.tf\n",
            "59M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights2.hdf5\n",
            "0\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1 (1).csv\n",
            "1.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs2.csv\n",
            "231M\t/content/drive/MyDrive/MnistResults/vggtrain_pruned_model2.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vggtrain_prunedStripped_basemodel2.tf\n",
            "171M\t/content/drive/MyDrive/MnistResults/vggtrain_save_basemodel2.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3gw14B4qRQd"
      },
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brhAe_Kqqhtz",
        "outputId": "1727e752-a919-4eea-a161-0efb4435043e"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "#print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpr524givv/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpr524givv/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved quantized and pruned TFLite model to: /tmp/tmpr9lefqp3.tflite\n",
            "Size of gzipped pruned and quantized TFlite model: 7155959.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi-tGrDBqQ61"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J40todFqu-8",
        "outputId": "e0b3b280-ea7e-492a-a942-b5ffb575a243"
      },
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(\"/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\")))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(\"/content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\")))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(tflite_model_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 236.00 bytes\n",
            "Size of gzipped pruned Keras model: 210.00 bytes\n",
            "Size of gzipped pruned TFlite model: 11521622.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqR-TGl5tO2c"
      },
      "source": [
        "# Quantized, Pruned and Zipped = 11.5M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxZHV2KQr5Qm",
        "outputId": "7c5b0a1a-4dd8-4aa1-c3c5-f4ddf06b0514"
      },
      "source": [
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/vgg32X32_P&Q_1_noZip.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8wnrcfe2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8wnrcfe2/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482XG1Ams3rT"
      },
      "source": [
        "# Quantized and Pruned tflite model VGG 32X32 = 15M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYJDy30FvLdb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_X[:1000]):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels[:1000]).mean()\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dX-QGlKvPfe",
        "outputId": "061fa3ad-4447-417c-ccc9-ef257f72431d"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated on 0 results so far.\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.993\n",
            "Pruned TF test accuracy: 0.9932000041007996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfDAT_SSV6ew"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO33IURcNTjA"
      },
      "source": [
        "# ================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmrMaSwgNNwQ"
      },
      "source": [
        "# Capsule Networn 32X32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPo9y0qZNe1q"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 8 * 8  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "tf.random.set_seed(500000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aajiBEV4Njrj"
      },
      "source": [
        "#class SquashLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SquashLayer, self).get_config()\n",
        "    config.update({\"axis\": self.axis})\n",
        "    return config\n",
        "\n",
        " \n",
        "#class SafeNorm(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SafeNorm(K.layers.Layer):\n",
        "  \n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    self.keep_dims = keep_dims\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, input):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims= self.keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SafeNorm, self).get_config()\n",
        "    config.update({\"axis\": self.axis, \"keep_dims\": self.keep_dims})\n",
        "    return config\n",
        "  \n",
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "  \n",
        "  def get_config(self):\n",
        "    config =  super(MyDigitCapsLayer, self).get_config()\n",
        "    return config\n",
        "  \n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1  # TODO: use\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\",\\\n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\\\n",
        "                      initializer=\"random_normal\",\\\n",
        "                      dtype=tf.float32)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def get_prunable_weights(self):\n",
        "    return [self.kernel]\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    return caps2_predicted\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "\n",
        "  \n",
        "    def get_config(self):\n",
        "      config = super(MarginLoss, self).get_config()\n",
        "      return config\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    \"\"\"\n",
        "    def get_prunable_weights(self):\n",
        "      return []\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self,y_true, input):\n",
        "      # print(f\"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "      # return K.losses.MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "      #y_true = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "      m_plus = 0.9\n",
        "      m_minus = 0.1\n",
        "      lambda_ = 0.5 \n",
        "      \n",
        "      #y_true one hot encode y_train\n",
        "      T = tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      \n",
        "      caps2_output_norm = self.safe_norm(input, keep_dims = True)\n",
        "\n",
        "      present_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                                    name=\"present_error_raw\")\n",
        "      present_error = tf.reshape(\\\n",
        "                                    present_error_raw, shape=(-1, 10),\n",
        "                                    name=\"present_error\")  \n",
        "  \n",
        "      absent_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                                    name=\"absent_error_raw\")\n",
        "      absent_error = tf.reshape(\\\n",
        "                                    absent_error_raw, shape=(-1, 10),\n",
        "                                    name=\"absent_error\")\n",
        "  \n",
        "      L = tf.add(\\\n",
        "                  T * present_error,\\\n",
        "                  lambda_ * (1.0 - T) * absent_error,\n",
        "                  name=\"L\")\n",
        "      marginLoss = tf.reduce_mean(\\\n",
        "                                  tf.reduce_sum(L, axis=1),\\\n",
        "                                  name=\"margin_loss\")\n",
        "      return marginLoss\n",
        "\n",
        "\n",
        "#class RoutingByAgreement(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, round_number=-1, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = round_number \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(RoutingByAgreement, self).get_config()\n",
        "    config.update({\"round_number\": self.round_number})\n",
        "    return config\n",
        " \n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "  \n",
        "  def get_prunable_weights(self):\n",
        "    return [self.raw_weights_1]\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def squash(inputs):\n",
        "    EPSILON = 1.0e-5\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def single_round_routing(self, inputs, raw_weights, agreement):\n",
        "    raw_weights = tf.add(raw_weights, agreement)\n",
        "    routing_wt = tf.nn.softmax(raw_weights, axis=2)\n",
        "    wt_predictions = tf.multiply(routing_wt, inputs)\n",
        "    wt_sum = tf.reduce_sum(wt_predictions, axis=1, keepdims=True)\n",
        "    return wt_sum\n",
        "\n",
        "  def call(self, inputs):\n",
        "    agreement = tf.zeros(shape=self.raw_weights_1.shape)\n",
        "    sqsh_wt_sum = None\n",
        "    x = inputs\n",
        "    for i in range(2):\n",
        "      wt_sum = self.single_round_routing(inputs, self.raw_weights_1, agreement)\n",
        "      sqsh_wt_sum = RoutingByAgreement.squash(wt_sum)\n",
        "      sqsh_wt_sum_tiled = tf.tile(\\\n",
        "                          sqsh_wt_sum ,\\\n",
        "                          [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                          name=\"caps2_output_round_1_tiled\")\n",
        "      agreement = tf.matmul(\\\n",
        "                            x, \\\n",
        "                            sqsh_wt_sum_tiled,\\\n",
        "                            transpose_a=True,\\\n",
        "                            name=\"agreement\")\n",
        "    return sqsh_wt_sum\n",
        "\n",
        "class MyAccuracy(K.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyAccuracy, self).__init__(**kwargs)\n",
        "    self.acc_obj = K.metrics.Accuracy()\n",
        "    self.state = 0\n",
        "  \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyAccuracy, self).get_config()\n",
        "    config.update({\"acc_obj\": None, \"state\": self.state})\n",
        "    return config\n",
        "  \n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def update_state(self, y_true, input, sample_weight=None):\n",
        "    if self.acc_obj is None:\n",
        "      self.acc_obj = K.metrics.Accuracy()\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    #y_true = tf.reshape(y_true, (y_true.shape[0], ))\n",
        "    y_true = tf.cast(y_true, dtype=tf.int64)\n",
        "    self.acc_obj.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.acc_obj.reset_state()\n",
        "\n",
        "  def result(self):\n",
        "    return self.acc_obj.result()\n",
        "\n",
        "class MyReshapeLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(MyReshapeLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-5, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def call(self, input):\n",
        "    print('printing shapes ------------------- ')\n",
        "    EPSILON = 1.0e-9\n",
        "    print(input)\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    print(y_proba)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    print(y_proba_argmax)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    print(y_pred)\n",
        "    return tf.cast(y_pred, tf.int64)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyReshapeLayer, self).get_config()\n",
        "    return config\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lxAttELN2QT"
      },
      "source": [
        "import keras\n",
        "tf.random.set_seed(500000)\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = K.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "train_X =[]\n",
        "\n",
        "for i in range(train_images.shape[0]):\n",
        "  train_pad = np.pad(train_images[i], pad_width=2 , mode= 'edge')\n",
        "  #print(train_pad)\n",
        "  train_X.append(train_pad)\n",
        "#PADDING test images\n",
        "test_X = []\n",
        "for i in range(test_images.shape[0]):\n",
        "  test_pad = np.pad(test_images[i], pad_width=2 , mode= 'edge')\n",
        "  #print(train_pad)\n",
        "  test_X.append(test_pad)\n",
        "train_X = np.array(train_X)\n",
        "test_X = np.array(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iaJK71AOASI",
        "outputId": "b7d23f4f-b91b-4ccd-e660-b3b4a333a7b4"
      },
      "source": [
        "x_train = train_X\n",
        "x_test = test_X\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 32, 32), (10000, 32, 32))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4SlifJZOE11",
        "outputId": "b6f543ff-5e53-47d1-e144-ae90094ddd97"
      },
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def build(inshape=(32, 32, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape(\\\n",
        "                             (caps1_n_caps, caps1_n_dims),\\\n",
        "                             name=\"reshape_layer_1\")(x)\n",
        "                             \n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement (2 rounds)\n",
        "        x = RoutingByAgreement(name=\"routing1\", round_number=2)(x)\n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "print(m.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 32, 32, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 24, 24, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 8, 8, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 2048, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 2048, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 2048, 10, 16, 1)   2621440   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      20480     \n",
            "=================================================================\n",
            "Total params: 7,971,584\n",
            "Trainable params: 7,971,584\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHcZnmbhOMs0"
      },
      "source": [
        "m.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKqFhyBHP0Ha",
        "outputId": "d422737c-6109-4ed3-a868-9fe59ebd1a66"
      },
      "source": [
        "import time\n",
        "class TimeHistory(K.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "time_callback = TimeHistory()\n",
        "history = m.fit(x_train, train_labels, batch_size=32, epochs=10, verbose= 1, validation_split=0.1, callbacks = [time_callback])\n",
        "times = time_callback.times\n",
        "print(times)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 224s 132ms/step - loss: 2.2778 - my_accuracy: 0.9248 - val_loss: 2.2755 - val_my_accuracy: 0.9653\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 219s 130ms/step - loss: 2.2778 - my_accuracy: 0.9370 - val_loss: 2.2755 - val_my_accuracy: 0.9720\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 218s 129ms/step - loss: 2.2778 - my_accuracy: 0.9439 - val_loss: 2.2756 - val_my_accuracy: 0.9772\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9293 - val_loss: 2.2755 - val_my_accuracy: 0.9593\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9461 - val_loss: 2.2754 - val_my_accuracy: 0.9807\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9455 - val_loss: 2.2757 - val_my_accuracy: 0.9310\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9358 - val_loss: 2.2753 - val_my_accuracy: 0.9780\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9463 - val_loss: 2.2756 - val_my_accuracy: 0.9718\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9400 - val_loss: 2.2755 - val_my_accuracy: 0.9740\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 217s 129ms/step - loss: 2.2778 - my_accuracy: 0.9478 - val_loss: 2.2757 - val_my_accuracy: 0.8897\n",
            "[224.1076259613037, 218.74299263954163, 218.09373426437378, 217.45154881477356, 217.09544372558594, 217.3020875453949, 217.18077993392944, 217.22720456123352, 217.07295417785645, 217.24898266792297]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYqiylsrYVdp",
        "outputId": "f7565f72-5f9e-4760-ba12-19d864ca7b24"
      },
      "source": [
        "average = sum(times)/len(times)\n",
        "print(average)\n",
        "print(\"training time per instance for capsule network : \" + str((average/54000)*1000)+ \" ms\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "218.1523354291916\n",
            "training time per instance for capsule network : 4.039858063503548 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox6Ns16poGq3",
        "outputId": "095b9412-be52-4551-f91c-82f5edba5497"
      },
      "source": [
        "import time\n",
        "class TimeHistory(K.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "time_callback = TimeHistory()\n",
        "history = m.fit(x_train, train_labels, batch_size=32, epochs=2, verbose= 1, validation_split=0.1, callbacks = [time_callback])\n",
        "times = time_callback.times\n",
        "print(times)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1688/1688 [==============================] - 233s 130ms/step - loss: 2.2778 - my_accuracy: 0.9252 - val_loss: 2.2755 - val_my_accuracy: 0.9652\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 218s 129ms/step - loss: 2.2778 - my_accuracy: 0.9369 - val_loss: 2.2755 - val_my_accuracy: 0.9702\n",
            "[233.21886086463928, 218.15961813926697]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSqHob_uOY7h"
      },
      "source": [
        "m.load_weights(\"/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGmfrSWUOodK",
        "outputId": "7c57c8f5-6e17-43e5-c3c1-2a87f957fa32"
      },
      "source": [
        "m.save(\"/content/drive/MyDrive/MnistResults/caps32X32_basemodel_3.tf\", save_format='tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/caps32X32_basemodel_3.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhdZMfaMqord",
        "outputId": "81521669-a40d-4676-909c-5857139b0702"
      },
      "source": [
        "m.evaluate(x_test, test_labels, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 19s 61ms/step - loss: 2.2765 - my_accuracy: 0.9745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.276534080505371, 0.9745000004768372]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rccAIaEOq7np",
        "outputId": "7e6d6985-2840-49e1-c93c-16a68b70f0a3"
      },
      "source": [
        "print(\"time for caps model evaluation: \" + str((19/test_X.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time for caps model evaluation: 1.9ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ZGktQprbUv"
      },
      "source": [
        "# pruning 65%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wGv9ZWZrdr_",
        "outputId": "3f24ebac-964c-49aa-f0f5-5fa8b885ad21"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# TODO: Remove this line\n",
        "#m.load_weights('/content/drive/MyDrive/MnistResults/best_weights4.hdf5')\n",
        "\n",
        "# print(\"ORIGINAL MODEL\")\n",
        "# print(mm.summary())\n",
        "# print('-' * 80)\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = x_train.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "mm = m\n",
        "mm.load_weights('/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5')\n",
        "\n",
        "# Define model for pruning.\n",
        "# pruning_params = {\n",
        "#       'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "#                                                                final_sparsity=0.80,\n",
        "#                                                                begin_step=0,\n",
        "#                                                                end_step=end_step)\n",
        "# }\n",
        "\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.20,\n",
        "                                                              final_sparsity=0.65,\n",
        "                                                              begin_step=0,\n",
        "                                                              end_step=end_step)\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "model_for_pruning = prune_low_magnitude(mm, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "######################################################################\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "# Helper function uses `prune_low_magnitude` to make only the \n",
        "# Dense layers train with pruning.\n",
        "def apply_pruning_to_layers(layer):\n",
        "  #print(\"called\")\n",
        "  if isinstance(layer, MyDigitCapsLayer):\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"conv_layer_2\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"routing1\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "  elif layer.name == \"conv_layer_1\":\n",
        "    print(f\"Layer {layer} {layer.name} slated for pruning\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
        "\n",
        "\n",
        "  print(f\"Layer {layer} {layer.name} unchanged\")\n",
        "  return layer\n",
        "\n",
        "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_layers` \n",
        "# to the layers of the model.\n",
        "model_for_pruning = tf.keras.models.clone_model(\n",
        "    mm,\n",
        "    clone_function=apply_pruning_to_layers,\n",
        ")\n",
        "#print(model_for_pruning.summary())\n",
        "\n",
        "\"\"\"\n",
        "model_for_pruning = K.models.Sequential(\\\n",
        "              [\\\n",
        "                model_for_pruning,\\\n",
        "                MyReshapeLayer(),\\\n",
        "              ]\\\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model_for_pruning.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "#model_for_pruning.compile(optimizer='adam', loss=MarginLoss())\n",
        "\n",
        "model_for_pruning.fit(x_train, train_labels,\n",
        "                  batch_size=32, epochs=2, validation_split=validation_split,\n",
        "                  callbacks=callbacks, verbose = 1)\n",
        "#_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer <keras.layers.convolutional.Conv2D object at 0x7f2121555790> conv_layer_1 slated for pruning\n",
            "Layer <keras.layers.convolutional.Conv2D object at 0x7f2121570690> conv_layer_2 slated for pruning\n",
            "Layer <keras.layers.core.Reshape object at 0x7f2121122cd0> reshape_layer_1 unchanged\n",
            "Layer <__main__.SquashLayer object at 0x7f212144fcd0> caps1_output_layer unchanged\n",
            "Layer <__main__.MyDigitCapsLayer object at 0x7f2121093a50> caps2_predicted slated for pruning\n",
            "Layer <__main__.RoutingByAgreement object at 0x7f2121011810> routing1 slated for pruning\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2223: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1688/1688 [==============================] - 225s 132ms/step - loss: 2.2778 - my_accuracy_3: 0.9310 - val_loss: 2.2755 - val_my_accuracy_3: 0.9643\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 218s 129ms/step - loss: 2.2778 - my_accuracy_3: 0.9442 - val_loss: 2.2755 - val_my_accuracy_3: 0.9723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20c4f46390>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G809RmrwtwX-",
        "outputId": "0d3b947a-bfec-4d54-dbc2-d9777694861d"
      },
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, test_labels, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 18s 58ms/step - loss: 2.2765 - my_accuracy_3: 0.9704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpohkZIBwBGe",
        "outputId": "db2239f5-4eb6-4c8f-93df-21fc6a98b9a7"
      },
      "source": [
        "print(\"evaluation time for prunned model : \" +str((18/x_test.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluation time for prunned model : 1.8ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np7NeNM-tfhb",
        "outputId": "7b374452-3899-4867-8699-cfd4df822186"
      },
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x_test, test_labels, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 18s 59ms/step - loss: 2.2765 - my_accuracy_3: 0.9704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJSFrA_wuT_",
        "outputId": "1a57eeb6-0dd9-4eab-825b-1c39b2f11aea"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()\n",
        "pruned_keras_file = \"/content/drive/MyDrive/MnistResults/caps32X32pruned_fileC.tf\"\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 32, 32, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 24, 24, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 8, 8, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 2048, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 2048, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 2048, 10, 16, 1)   2621440   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      20480     \n",
            "=================================================================\n",
            "Total params: 7,971,584\n",
            "Trainable params: 7,971,584\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/caps32X32pruned_fileC.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/MnistResults/caps32X32pruned_fileC.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved pruned Keras model to: /content/drive/MyDrive/MnistResults/caps32X32pruned_fileC.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4iUTitLwuQ-"
      },
      "source": [
        "model_for_export.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zk-bnMlxWrT",
        "outputId": "2e943eb7-a4c4-4664-e1d5-9039a5d17a90"
      },
      "source": [
        "model_for_export.evaluate(x_test, test_labels, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 19s 58ms/step - loss: 2.2765 - my_accuracy_4: 0.9704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.2765438556671143, 0.9703999757766724]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLgjWGuYxooB"
      },
      "source": [
        "#trying original weights on model_for_export\n",
        "model_check = model_for_export\n",
        "model_check.load_weights(\"/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCKKMw4Sx8EI",
        "outputId": "44c7839f-0970-4a3b-a415-bb103034126f"
      },
      "source": [
        "model_check.evaluate(x_test, test_labels, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 18s 59ms/step - loss: 2.2765 - my_accuracy_4: 0.9745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.276534080505371, 0.9745000004768372]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERsjes6yT72"
      },
      "source": [
        "# Now compression after pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46J4_qSJyW_N",
        "outputId": "85ffb188-6e43-40ed-8943-9394cb4a63d6"
      },
      "source": [
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpm7ovq2hb/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpm7ovq2hb/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI2QX6ltzW3b",
        "outputId": "de8686d9-8145-4687-e14a-328c8ae73ed5"
      },
      "source": [
        "tflite_model_file = \"/content/drive/MyDrive/MnistResults/caps32X32_P&Q_2_noZip.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8cuedoab/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8cuedoab/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fpTt0kp0AZy",
        "outputId": "fc3d0fcb-f28e-4a03-e408-9cd4d1f770cd"
      },
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(\"/content/drive/MyDrive/MnistResults/caps32X32_basemodel_1.tf\")))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(tflite_model_file)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 218.00 bytes\n",
            "Size of gzipped pruned Keras model: 218.00 bytes\n",
            "Size of gzipped pruned TFlite model: 14577587.00 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnXBl8gzqVYf",
        "outputId": "904d748c-e241-43c3-e92a-c6297e8b5297"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27M\t/content/drive/MyDrive/MnistResults/best_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/best_weights5.hdf5\n",
            "92M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_1.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_2.tf\n",
            "92M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_3.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_compressedB.tflite\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps32X32mylogs5.csv\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1_noZip.tflite\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1.tflite\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_2_noZip.tflite\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.h5\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileC.tf\n",
            "3.0K\t/content/drive/MyDrive/MnistResults/caps48X48mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/caps_basemodelA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelA.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelB.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_best_weightsA.hdf5\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedA.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedB.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_latest_weightsA.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps_mylogsA.csv\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1_noZip.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_save_basemodelA.tf\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_best_weights4.hdf5\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_latest_weights4.hdf5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/cnn_mylogs4.csv\n",
            "367K\t/content/drive/MyDrive/MnistResults/cnn_save_basemodel4.tf\n",
            "11M\t/content/drive/MyDrive/MnistResults/compressed1.tflite\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_best_weights1.hdf5\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_latest_weights1.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/dense_mylogs1.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights1.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights2.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights3.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights5.hdf5\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs2.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs3.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs4.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file1.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file2.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_30.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_60.h5\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode2.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode3.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel4.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel5.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg32X32basemodel.h5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg32X32_P&Q_1_noZip.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.h5\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_2.h5\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg_compressed32X32_1.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights4.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights4.hdf5\n",
            "2.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/vggfine_mylogs2.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs4.csv\n",
            "158M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel4.tf\n",
            "59M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights2.hdf5\n",
            "0\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1 (1).csv\n",
            "1.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs2.csv\n",
            "231M\t/content/drive/MyDrive/MnistResults/vggtrain_pruned_model2.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vggtrain_prunedStripped_basemodel2.tf\n",
            "171M\t/content/drive/MyDrive/MnistResults/vggtrain_save_basemodel2.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQl8HeTWPR3K",
        "outputId": "4602060b-a27b-4b55-bc8f-43ab2d323ec3"
      },
      "source": [
        "m.evaluate(test_X, test_labels, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 20s 61ms/step - loss: 2.2765 - my_accuracy: 0.9745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.276534080505371, 0.9745000004768372]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdABGvrPgI1",
        "outputId": "f50411b3-679c-4eeb-a606-432f71ef25e5"
      },
      "source": [
        "print(\"time for caps model evaluation: \" + str((20/test_X.shape[0])*1000) + \"ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time for caps model evaluation: 2.0ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXIjYTmPPjoi"
      },
      "source": [
        "# Caps 32X32 base model evaluation time 2ms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnQEc3AoOkPl",
        "outputId": "0b97618e-90aa-426c-b5f6-931a3e9b8a3d"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/MnistResults/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27M\t/content/drive/MyDrive/MnistResults/best_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/best_weights5.hdf5\n",
            "92M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_1.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_basemodel_2.tf\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32_best_weights5.hdf5\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_compressedB.tflite\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps32X32mylogs5.csv\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1_noZip.tflite\n",
            "16M\t/content/drive/MyDrive/MnistResults/caps32X32_P&Q_1.tflite\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.h5\n",
            "31M\t/content/drive/MyDrive/MnistResults/caps32X32pruned_fileB.tf\n",
            "3.0K\t/content/drive/MyDrive/MnistResults/caps48X48mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodel28X28_1.h5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/caps_basemodelA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelA.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_basemodelB.tf\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_best_weightsA.hdf5\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedA.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_compressedB.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_latest_weightsA.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/caps_mylogsA.csv\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1_noZip.tflite\n",
            "11M\t/content/drive/MyDrive/MnistResults/caps_P&Q_1.tflite\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileA.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_pruned_fileB.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/caps_save_basemodelA.tf\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_best_weights4.hdf5\n",
            "95K\t/content/drive/MyDrive/MnistResults/cnn_latest_weights4.hdf5\n",
            "1.0K\t/content/drive/MyDrive/MnistResults/cnn_mylogs4.csv\n",
            "367K\t/content/drive/MyDrive/MnistResults/cnn_save_basemodel4.tf\n",
            "11M\t/content/drive/MyDrive/MnistResults/compressed1.tflite\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_best_weights1.hdf5\n",
            "21M\t/content/drive/MyDrive/MnistResults/dense_latest_weights1.hdf5\n",
            "4.0K\t/content/drive/MyDrive/MnistResults/dense_mylogs1.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights1.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights2.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights3.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights4.hdf5\n",
            "27M\t/content/drive/MyDrive/MnistResults/latest_weights5.hdf5\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs2.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/mylogs3.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs4.csv\n",
            "5.5K\t/content/drive/MyDrive/MnistResults/mylogs5.csv\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file1.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file2.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_30.h5\n",
            "27M\t/content/drive/MyDrive/MnistResults/pruned_file4_60.h5\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode2.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemode3.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel4.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel5.tf\n",
            "79M\t/content/drive/MyDrive/MnistResults/save_basemodel.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg32X32basemodel.h5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg32X32_P&Q_1_noZip.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.h5\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_1.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_basemodel32X32_withWeights.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vgg_best_weightsB.hdf5\n",
            "15M\t/content/drive/MyDrive/MnistResults/vgg_compressed32X32_1.tflite\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_best_weights4.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights1.hdf5\n",
            "263K\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggfine_latest_weights4.hdf5\n",
            "2.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs1.csv\n",
            "512\t/content/drive/MyDrive/MnistResults/vggfine_mylogs2.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggfine_mylogs4.csv\n",
            "158M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel4.tf\n",
            "59M\t/content/drive/MyDrive/MnistResults/vggfine_save_basemodel.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vgg_pruned32X32_1.tf\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_best_weights2.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights1.hdf5\n",
            "57M\t/content/drive/MyDrive/MnistResults/vggtrain_latest_weights2.hdf5\n",
            "0\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1 (1).csv\n",
            "1.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs1.csv\n",
            "4.5K\t/content/drive/MyDrive/MnistResults/vggtrain_mylogs2.csv\n",
            "231M\t/content/drive/MyDrive/MnistResults/vggtrain_pruned_model2.tf\n",
            "58M\t/content/drive/MyDrive/MnistResults/vggtrain_prunedStripped_basemodel2.tf\n",
            "171M\t/content/drive/MyDrive/MnistResults/vggtrain_save_basemodel2.tf\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}