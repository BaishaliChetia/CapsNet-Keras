{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "b_mnistTf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaishaliChetia/CapsNet-Keras/blob/master/b_mnistTf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeoBe9bpMlPR"
      },
      "source": [
        "Original implementation at:\n",
        "\n",
        "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb\n",
        "\n",
        "Geron's model doesn't use the keras functional API. In the keras functional API, you don't need to give the batchsize. \n",
        "\n",
        "When you print the model, you get this:\n",
        "\n",
        "```\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "_________________________________________________________________\n",
        "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
        "_________________________________________________________________\n",
        "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
        "_________________________________________________________________\n",
        "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
        "_________________________________________________________________\n",
        "reshape_layer_1 (Reshape)    (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "caps1_output_layer (SquashLa (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "Total params: 5,329,664\n",
        "Trainable params: 5,329,664\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "Notice that the Input-layer has shape (None, 28, 28, 1), but we only specified (28, 28, 1). It added None implicitly and that takes care of the batch.\n",
        "\n",
        "So for anywhere Geron uses the batch size explicitly, you don't need to do anything and tensorflow will take care of.\n",
        "\n",
        "Also note that tensorflow 1 APIs are still provided with the compat layer. I used the reduce_sum from TF1 in the squash layer, that allowed me to use Geron's code.\n",
        "\n",
        "Documentation on how to migrate from TF1 to TF2 can be found here:\n",
        "\n",
        "https://www.tensorflow.org/guide/migrate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCmDd7lMlPU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UydL5gJMlPV"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukwGEY4MlPV"
      },
      "source": [
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    #print(\"SquashLayer input_shape=\", input_shapes)\n",
        "    pass\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qIVI_u2i-s"
      },
      "source": [
        "class SafeNorm(K.layers.Layer):\n",
        "  def __init__(self, axis=-1,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "  def call(self, input, keep_dims = False):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs), axis=self.axis, keepdims= keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECOObckMlPW"
      },
      "source": [
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1\n",
        "    #                                    print(\"input_shapes = \", input_shapes)\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\", \n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "                      initializer=\"random_normal\", dtype=tf.float32)\n",
        "    #print(\"My Caps Layer2: self.kernel = \", self.kernel.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def call(self, inputs):\n",
        "    #                                    print(\"==>\", inputs.shape)\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    #                                    print(\"exp1 ==>\", exp1.shape)\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    #                                    print(\"exp1 ==>\", exp1.shape)\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    #                                    print(\"tile ==> \", tile.shape)\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    #                                    print(\"multiply ==>\", caps2_predicted.shape)\n",
        "    return caps2_predicted"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXMBYOeWlDd"
      },
      "source": [
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, roundno=-1, agreement=None, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = roundno\n",
        "    self.agreement = agreement\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "\n",
        "  def squash(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "    \n",
        "  def single_round_routing(self, inputs):\n",
        "    pass\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if self.round_number == 1:\n",
        "      self.raw_weights = self.raw_weights_1\n",
        "    \n",
        "    elif self.round_number == 2:\n",
        "      self.raw_weights = tf.add(self.raw_weights_1,\\\n",
        "                                agreement,\\\n",
        "                                name=\"raw_weights_round_2\")\n",
        "    \n",
        "    value = inputs\n",
        "    for i in range(num_routes):\n",
        "      value = single_round_routing(self, value)\n",
        "\n",
        "    self.routing_weights = tf.nn.softmax(self.raw_weights,\\\n",
        "                                         axis=2,\\\n",
        "                                         name=\"routing_weights\")\n",
        "    \n",
        "    weighted_predictions = tf.multiply(self.routing_weights, inputs,\n",
        "                                   name=\"weighted_predictions\")\n",
        "    weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True,\n",
        "                             name=\"weighted_sum\")\n",
        "    \n",
        "    return weighted_sum\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Pg6qxAU3h0hv",
        "outputId": "3a010ae6-3dce-4410-c2e7-ea62e1690039"
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "    \n",
        "    def call(self, y_true, y_pred):\n",
        "      print(y_true, y_true.shape)\n",
        "      print(y_pred, y_pred.shape)\n",
        "      return 0\n",
        "\n",
        "\n",
        "def margin_loss(input): #input = caps2_output_round_2\n",
        "  y = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "  m_plus = 0.9\n",
        "  m_minus = 0.1\n",
        "  lambda_ = 0.5 \n",
        "  T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
        "  print(\"T:\", T)\n",
        "  caps2_output_norm = SafeNorm()(input, keep_dims = True)\n",
        "  \n",
        "  present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                              name=\"present_error_raw\")\n",
        "  present_error = tf.reshape(present_error_raw, shape=(-1, 10),\n",
        "                           name=\"present_error\")  \n",
        "  \n",
        "  absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                             name=\"absent_error_raw\")\n",
        "  absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),\n",
        "                          name=\"absent_error\")\n",
        "  \n",
        "  L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
        "           name=\"L\")\n",
        "  marginLoss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
        "  return marginLoss"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7318aa9cef16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMarginLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarginLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmSudqTMlPX",
        "outputId": "3bf3507c-6b88-4f15-934b-a4e6ef4423fd"
      },
      "source": [
        "\n",
        "\n",
        "class Model:\n",
        "    @staticmethod\n",
        "    def build(inshape=(28, 28, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape((caps1_n_caps, caps1_n_dims), name=\"reshape_layer_1\")(x)\n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        # The next part is to build the digit caps, starts from\n",
        "        # 4:38 in the video\n",
        "        # I haven't got to it yet, this will take time\n",
        "        # at any point of time print the shape, and it will print it\n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement\n",
        "        #Round 1\n",
        "        x = RoutingByAgreement(name=\"routing1\", roundno=1, agreement=None)(x)\n",
        "        x = SquashLayer(name= \"caps2_output_round_1\", axis= -2)(x)\n",
        "\n",
        "\n",
        "        #Round2\n",
        "        #tiling\n",
        "        x = tf.tile(\\\n",
        "                    x ,\\\n",
        "                    [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                    name=\"caps2_output_round_1_tiled\")\n",
        "        #agreement\n",
        "        agreement = tf.matmul(\\\n",
        "                              caps2_predicted, \\\n",
        "                              x,\\\n",
        "                              transpose_a=True,\\\n",
        "                              name=\"agreement\")\n",
        "        print(f\"x.shape = {x.shape}, caps2_predicted.shape = {caps2_predicted.shape}, agreement.shape = {agreement.shape}\")\n",
        "        \n",
        "        x = RoutingByAgreement(name=\"routing2\", roundno=2, agreement=agreement)(x)\n",
        "        x = SquashLayer(name= \"caps2_output_round_2\", axis= -2)(x)\n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "\n",
        "# Print the final shape\n",
        "print(m, type(m), m.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My Caps Layer2: self.kernel =  (1152, 10, 16, 8) input_shape =  (None, 1152, 8)\n",
            "Model: \"my\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_layer_1 (Conv2D)           (None, 20, 20, 256)  20992       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_layer_2 (Conv2D)           (None, 6, 6, 256)    5308672     conv_layer_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_layer_1 (Reshape)       (None, 1152, 8)      0           conv_layer_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "caps1_output_layer (SquashLayer (None, 1152, 8)      0           reshape_layer_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "caps2_predicted (MyDigitCapsLay (None, 1152, 10, 16, 1474560     caps1_output_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "routing1 (RoutingByAgreement)   (None, 1, 10, 16, 1) 11520       caps2_predicted[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "caps2_output_round_1 (SquashLay (None, 1, 10, 16, 1) 0           routing1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.tile (TFOpLambda)            (None, 1152, 10, 16, 0           caps2_output_round_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul (TFOpLambda)   (None, 1152, 10, 1,  0           caps2_predicted[0][0]            \n",
            "                                                                 tf.tile[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "routing2 (RoutingByAgreement)   (None, 1, 10, 16, 1) 11520       caps2_predicted[0][0]            \n",
            "                                                                 tf.linalg.matmul[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "caps2_output_round_2 (SquashLay (None, 1, 10, 16, 1) 0           routing2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,827,264\n",
            "Trainable params: 6,827,264\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7f5f6388b2d0> <class 'tensorflow.python.keras.engine.functional.Functional'> None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEe-231jn49"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3j_zWQMlPZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cahMgtOqMlPa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyUFx-dcMlPa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}