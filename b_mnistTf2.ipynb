{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "b_mnistTf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaishaliChetia/CapsNet-Keras/blob/master/b_mnistTf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeoBe9bpMlPR"
      },
      "source": [
        "Original implementation at:\n",
        "\n",
        "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb\n",
        "\n",
        "Geron's model doesn't use the keras functional API. In the keras functional API, you don't need to give the batchsize. \n",
        "\n",
        "When you print the model, you get this:\n",
        "\n",
        "```\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "_________________________________________________________________\n",
        "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
        "_________________________________________________________________\n",
        "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
        "_________________________________________________________________\n",
        "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
        "_________________________________________________________________\n",
        "reshape_layer_1 (Reshape)    (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "caps1_output_layer (SquashLa (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "Total params: 5,329,664\n",
        "Trainable params: 5,329,664\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "Notice that the Input-layer has shape (None, 28, 28, 1), but we only specified (28, 28, 1). It added None implicitly and that takes care of the batch.\n",
        "\n",
        "So for anywhere Geron uses the batch size explicitly, you don't need to do anything and tensorflow will take care of.\n",
        "\n",
        "Also note that tensorflow 1 APIs are still provided with the compat layer. I used the reduce_sum from TF1 in the squash layer, that allowed me to use Geron's code.\n",
        "\n",
        "Documentation on how to migrate from TF1 to TF2 can be found here:\n",
        "\n",
        "https://www.tensorflow.org/guide/migrate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCmDd7lMlPU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UydL5gJMlPV"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukwGEY4MlPV"
      },
      "source": [
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    #print(\"SquashLayer input_shape=\", input_shapes)\n",
        "    pass\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qIVI_u2i-s"
      },
      "source": [
        "class SafeNorm(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    self.keep_dims = keep_dims\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "  def call(self, input):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs), axis=self.axis, keepdims= self.keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECOObckMlPW"
      },
      "source": [
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1\n",
        "    #                                    print(\"input_shapes = \", input_shapes)\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\", \n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
        "                      initializer=\"random_normal\",  dtype=tf.float32)\n",
        "    #print(\"My Caps Layer2: self.kernel = \", self.kernel.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def call(self, inputs):\n",
        "    #                                    print(\"==>\", inputs.shape)\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    #                                    print(\"exp1 ==>\", exp1.shape)\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    #                                    print(\"exp1 ==>\", exp1.shape)\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    #                                    print(\"tile ==> \", tile.shape)\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    #                                    print(\"multiply ==>\", caps2_predicted.shape)\n",
        "    return caps2_predicted"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zxRXCYIz_HQ"
      },
      "source": [
        "class MyAccuracy(K.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyAccuracy, self).__init__(**kwargs)\n",
        "    self.state = 0\n",
        "  \n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "  def update_state(self, y_true, input, sample_weight=None):\n",
        "    try:\n",
        "      print(\"--__-->\", y_true.shape, input.shape)\n",
        "      #tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      y_proba = self.safe_norm(input, axis=-2)\n",
        "      y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "      y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "      y_true = tf.reshape(y_true, (y_true.shape[0], ))\n",
        "\n",
        "      y_true = tf.cast(y_true, dtype=tf.int64)\n",
        "      y_same = tf.cast(tf.math.equal(y_true, y_pred), dtype=tf.int32)\n",
        "      y_same = tf.math.reduce_mean(y_same, keepdims=True)\n",
        "      self.state = y_same\n",
        "      print(\"-->\", y_same)\n",
        "      #print(\"|||-->\", tf.make_ndarray(y_same))\n",
        "    except:\n",
        "      self.state = 0\n",
        "    \n",
        "\n",
        "  def result(self):\n",
        "    return self.state"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg6qxAU3h0hv"
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    \n",
        "\n",
        "    def call(self,y_true, input):\n",
        "      # print(f\"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "      # return K.losses.MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "      #y_true = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "      m_plus = 0.9\n",
        "      m_minus = 0.1\n",
        "      lambda_ = 0.5 \n",
        "      print(\"-->\", y_true.shape, input.shape)\n",
        "\n",
        "     \n",
        "      \n",
        "      \n",
        "      #y_true one hot encode y_train\n",
        "      T = tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      \n",
        "      caps2_output_norm = self.safe_norm(input, keep_dims = True)\n",
        "\n",
        "      present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                              name=\"present_error_raw\")\n",
        "      present_error = tf.reshape(present_error_raw, shape=(-1, 10),\n",
        "                           name=\"present_error\")  \n",
        "  \n",
        "      absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                             name=\"absent_error_raw\")\n",
        "      absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),\n",
        "                          name=\"absent_error\")\n",
        "  \n",
        "      L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
        "           name=\"L\")\n",
        "      marginLoss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
        "      return marginLoss\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXMBYOeWlDd"
      },
      "source": [
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, roundno=-1, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = roundno\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "\n",
        "  def squash(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           #axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def single_round_routing(self, inputs, raw_weights, agreement):\n",
        "    raw_weights = tf.add(raw_weights, agreement)\n",
        "    routing_wt = tf.nn.softmax(raw_weights, axis=2)\n",
        "    wt_predictions = tf.multiply(routing_wt, inputs)\n",
        "    wt_sum = tf.reduce_sum(wt_predictions, axis=1, keepdims=True)\n",
        "    return wt_sum\n",
        "\n",
        "  def call(self, inputs):\n",
        "    #TODO: There might be some gymnastics with the shape below\n",
        "    agreement = tf.zeros(shape=self.raw_weights_1.shape)\n",
        "    sqsh_wt_sum = None\n",
        "    x = inputs\n",
        "    for i in range(2):\n",
        "      wt_sum = self.single_round_routing(inputs, self.raw_weights_1, agreement)\n",
        "      sqsh_wt_sum = self.squash(wt_sum)\n",
        "      sqsh_wt_sum_tiled = tf.tile(\\\n",
        "                          sqsh_wt_sum ,\\\n",
        "                          [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                          name=\"caps2_output_round_1_tiled\")\n",
        "      agreement = tf.matmul(\\\n",
        "                            x, \\\n",
        "                            sqsh_wt_sum_tiled,\\\n",
        "                            transpose_a=True,\\\n",
        "                            name=\"agreement\")\n",
        "    return sqsh_wt_sum                           \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEe-231jn49"
      },
      "source": [
        "(x_train, y_train,), (x_test, y_test) = K.datasets.mnist.load_data()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmSudqTMlPX",
        "outputId": "e389ef20-03f7-404b-85e9-12eeaafcc611"
      },
      "source": [
        "\n",
        "\n",
        "class Model:\n",
        "  \n",
        "\n",
        "    @staticmethod\n",
        "    def build(inshape=(28, 28, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape((caps1_n_caps, caps1_n_dims), name=\"reshape_layer_1\")(x)\n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        # The next part is to build the digit caps, starts from\n",
        "        # 4:38 in the video\n",
        "        # I haven't got to it yet, this will take time\n",
        "        # at any point of time print the shape, and it will print it\n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement\n",
        "        #Round 1\n",
        "        x = RoutingByAgreement(name=\"routing1\", roundno=1)(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        \n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "\n",
        "# Print the final shape\n",
        "print(m, type(m), m.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method RoutingByAgreement.squash of <__main__.RoutingByAgreement object at 0x7fd2619b6710>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: invalid syntax (<unknown>, line 5)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method RoutingByAgreement.squash of <__main__.RoutingByAgreement object at 0x7fd2619b6710>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: invalid syntax (<unknown>, line 5)\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "(None, 1, 10, 16, 1)\n",
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7fd269edc610> <class 'tensorflow.python.keras.engine.functional.Functional'> None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3j_zWQMlPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b454c8ad-43fc-448e-cf08-abc5d00aa9df"
      },
      "source": [
        "y_train_train = tf.one_hot(y_train, depth=caps2_n_caps, name=\"T\")\n",
        "print(y_train_train.shape)\n",
        "#print(y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cahMgtOqMlPa",
        "outputId": "8fb7ec02-6b49-4230-ff02-e355167071e7"
      },
      "source": [
        "m.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])#, metrics=['accuracy'])\n",
        "history = m.fit(x_train, y_train, batch_size= 32, epochs= 3, verbose= 2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "--> (32, 1) (32, 1, 10, 16, 1)\n",
            "--__--> (32, 1) (32, 1, 10, 16, 1)\n",
            "--> Tensor(\"Mean:0\", shape=(1,), dtype=int32)\n",
            "--> (32, 1) (32, 1, 10, 16, 1)\n",
            "--__--> (32, 1) (32, 1, 10, 16, 1)\n",
            "--> Tensor(\"Mean:0\", shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyUFx-dcMlPa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}