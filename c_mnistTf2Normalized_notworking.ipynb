{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "c_mnistTf2Normalized.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaishaliChetia/CapsNet-Keras/blob/master/c_mnistTf2Normalized_notworking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeoBe9bpMlPR"
      },
      "source": [
        "Original implementation at:\n",
        "\n",
        "https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb\n",
        "\n",
        "Geron's model doesn't use the keras functional API. In the keras functional API, you don't need to give the batchsize. \n",
        "\n",
        "When you print the model, you get this:\n",
        "\n",
        "```\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "_________________________________________________________________\n",
        "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
        "_________________________________________________________________\n",
        "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
        "_________________________________________________________________\n",
        "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
        "_________________________________________________________________\n",
        "reshape_layer_1 (Reshape)    (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "caps1_output_layer (SquashLa (None, 1, 1152, 8)        0         \n",
        "_________________________________________________________________\n",
        "Total params: 5,329,664\n",
        "Trainable params: 5,329,664\n",
        "Non-trainable params: 0\n",
        "```\n",
        "\n",
        "Notice that the Input-layer has shape (None, 28, 28, 1), but we only specified (28, 28, 1). It added None implicitly and that takes care of the batch.\n",
        "\n",
        "So for anywhere Geron uses the batch size explicitly, you don't need to do anything and tensorflow will take care of.\n",
        "\n",
        "Also note that tensorflow 1 APIs are still provided with the compat layer. I used the reduce_sum from TF1 in the squash layer, that allowed me to use Geron's code.\n",
        "\n",
        "Documentation on how to migrate from TF1 to TF2 can be found here:\n",
        "\n",
        "https://www.tensorflow.org/guide/migrate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY0OS1x1qLp2",
        "outputId": "b5240ff4-4e6d-4cd8-9f4f-1a92e670ff6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCmDd7lMlPU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UydL5gJMlPV"
      },
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "caps2_n_caps = 10\n",
        "caps2_n_dims = 16\n",
        "\n",
        "tf.random.set_seed(500000)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukwGEY4MlPV"
      },
      "source": [
        "#class SquashLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SquashLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, **kwargs):\n",
        "    super(SquashLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    \n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SquashLayer, self).get_config()\n",
        "    config.update({\"axis\": self.axis})\n",
        "    return config\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qIVI_u2i-s"
      },
      "source": [
        "#class SafeNorm(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class SafeNorm(K.layers.Layer):\n",
        "  \n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(SafeNorm, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "    self.keep_dims = keep_dims\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return []\n",
        "  \"\"\"\n",
        "\n",
        "  def call(self, input):\n",
        "    EPSILON = 1.0e-9\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           axis=self.axis,\\\n",
        "                                           keepdims= self.keep_dims)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    return safe_norm\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(SafeNorm, self).get_config()\n",
        "    config.update({\"axis\": self.axis, \"keep_dims\": self.keep_dims})\n",
        "    return config\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECOObckMlPW"
      },
      "source": [
        "# This should be the part where the digit layer, and where we tile things\n",
        "# This is incomplete, and work in progress\n",
        "# TODO: Complete this\n",
        "class MyDigitCapsLayer(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyDigitCapsLayer, self).__init__(**kwargs)\n",
        "  \n",
        "  def get_config(self):\n",
        "    config =  super(MyDigitCapsLayer, self).get_config()\n",
        "    return config\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    init_sigma = 0.1  # TODO: use\n",
        "    self.kernel = self.add_weight(\\\n",
        "                      \"kernel\",\\\n",
        "                      (caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\\\n",
        "                      initializer=\"random_normal\",\\\n",
        "                      dtype=tf.float32)\n",
        "\n",
        "  \n",
        "  # To debug this function, I used prints to print the shape\n",
        "  # expand_dims just adds an exis, so if you say expand_dims(inshape=(5, 3), -1),\n",
        "  # you get the output shape (5, 3, 1), it just adds an axis at the end\n",
        "  # Then tile just multiplies one of the dimensions (that is it stacks along that direction N times)\n",
        "  # so tile(inshape=(5, 3, 1), [1, 1, 1000]) will yield a shape (5, 3, 1000)\n",
        "  #\n",
        "  # Notice I didn't tile in build, but in call, Most probaly this is the right thing to do\n",
        "  # but we'll only figure out when we actually train\n",
        "  def get_prunable_weights(self):\n",
        "    return [self.kernel]\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # Add a dimension at the end\n",
        "    exp1 = tf.expand_dims(inputs, -1, name=\"caps1_output_expanded\")\n",
        "    # add a dimension along 3rd axis\n",
        "    exp1 = tf.expand_dims(exp1, 2, name=\"caps2_output_espanced\")\n",
        "    # tile along 3rd axis\n",
        "    tile = tf.tile(exp1, [1, 1, caps2_n_caps, 1, 1], name=\"caps1_output_tiled\")\n",
        "    caps2_predicted = tf.matmul(self.kernel, tile, name=\"caps2_predicted\")\n",
        "    return caps2_predicted\n",
        "\n",
        "  \n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg6qxAU3h0hv"
      },
      "source": [
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss\n",
        "class MarginLoss(K.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(MarginLoss, self).__init__(**kwargs)\n",
        "\n",
        "  \n",
        "    def get_config(self):\n",
        "      config = super(MarginLoss, self).get_config()\n",
        "      return config\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    \"\"\"\n",
        "    def get_prunable_weights(self):\n",
        "      return []\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self,y_true, input):\n",
        "      # print(f\"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "      # return K.losses.MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "      #y_true = K.Input(shape=[], dtype=tf.int64, name=\"y\")\n",
        "      m_plus = 0.9\n",
        "      m_minus = 0.1\n",
        "      lambda_ = 0.5 \n",
        "      \n",
        "      #y_true one hot encode y_train\n",
        "      T = tf.one_hot(y_true, depth=caps2_n_caps, name=\"T\")\n",
        "      \n",
        "      caps2_output_norm = self.safe_norm(input, keep_dims = True)\n",
        "\n",
        "      present_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., m_plus - caps2_output_norm),\n",
        "                                    name=\"present_error_raw\")\n",
        "      present_error = tf.reshape(\\\n",
        "                                    present_error_raw, shape=(-1, 10),\n",
        "                                    name=\"present_error\")  \n",
        "  \n",
        "      absent_error_raw = tf.square(\\\n",
        "                                    tf.maximum(0., caps2_output_norm - m_minus),\n",
        "                                    name=\"absent_error_raw\")\n",
        "      absent_error = tf.reshape(\\\n",
        "                                    absent_error_raw, shape=(-1, 10),\n",
        "                                    name=\"absent_error\")\n",
        "  \n",
        "      L = tf.add(\\\n",
        "                  T * present_error,\\\n",
        "                  lambda_ * (1.0 - T) * absent_error,\n",
        "                  name=\"L\")\n",
        "      marginLoss = tf.reduce_mean(\\\n",
        "                                  tf.reduce_sum(L, axis=1),\\\n",
        "                                  name=\"margin_loss\")\n",
        "      return marginLoss\n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXMBYOeWlDd"
      },
      "source": [
        "#class RoutingByAgreement(K.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
        "class RoutingByAgreement(K.layers.Layer):\n",
        "  def __init__(self, round_number=-1, **kwargs):\n",
        "    super(RoutingByAgreement, self).__init__(**kwargs)\n",
        "    self.round_number = round_number \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(RoutingByAgreement, self).get_config()\n",
        "    config.update({\"round_number\": self.round_number})\n",
        "    return config\n",
        "\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    self.raw_weights_1 = self.add_weight(\"raw_weights\", \\\n",
        "                                         (caps1_n_caps, caps2_n_caps, 1, 1), \\\n",
        "                                         initializer = \"zeros\", \\\n",
        "                                         dtype=tf.float32,)\n",
        "    \n",
        "    #print(\"Routing layer: self.raw_weights = \", self.raw_weights.shape, \"input_shape = \", input_shapes)\n",
        "\n",
        "  \"\"\"\n",
        "  def get_prunable_weights(self):\n",
        "    return [self.raw_weights_1]\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def squash(inputs):\n",
        "    EPSILON = 1.0e-7\n",
        "    squared_norm = tf.compat.v1.reduce_sum(tf.square(inputs),\\\n",
        "                                           keepdims=True)\n",
        "    safe_norm = tf.sqrt(squared_norm + EPSILON)\n",
        "    squash_factor = squared_norm / (1. + squared_norm)\n",
        "    unit_vector = inputs / safe_norm\n",
        "    return squash_factor * unit_vector\n",
        "\n",
        "  def single_round_routing(self, inputs, raw_weights, agreement):\n",
        "    raw_weights = tf.add(raw_weights, agreement)\n",
        "    routing_wt = tf.nn.softmax(raw_weights, axis=2)\n",
        "    wt_predictions = tf.multiply(routing_wt, inputs)\n",
        "    wt_sum = tf.reduce_sum(wt_predictions, axis=1, keepdims=True)\n",
        "    return wt_sum\n",
        "\n",
        "  def call(self, inputs):\n",
        "    agreement = tf.zeros(shape=self.raw_weights_1.shape)\n",
        "    sqsh_wt_sum = None\n",
        "    x = inputs\n",
        "    for i in range(2):\n",
        "      wt_sum = self.single_round_routing(inputs, self.raw_weights_1, agreement)\n",
        "      sqsh_wt_sum = RoutingByAgreement.squash(wt_sum)\n",
        "      sqsh_wt_sum_tiled = tf.tile(\\\n",
        "                          sqsh_wt_sum ,\\\n",
        "                          [1, caps1_n_caps, 1, 1, 1],\\\n",
        "                          name=\"caps2_output_round_1_tiled\")\n",
        "      agreement = tf.matmul(\\\n",
        "                            x, \\\n",
        "                            sqsh_wt_sum_tiled,\\\n",
        "                            transpose_a=True,\\\n",
        "                            name=\"agreement\")\n",
        "    return sqsh_wt_sum                           \n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEe-231jn49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42134220-e57e-45be-96f1-3272ae7069fa"
      },
      "source": [
        "(x_train, y_train,), (x_test, y_test) = K.datasets.mnist.load_data()\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcHaaMo8db4K"
      },
      "source": [
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "\n",
        "#print(x_train[500])"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnmSudqTMlPX",
        "outputId": "9d45617f-f3d3-4ccd-996a-9a8f5f1134f3"
      },
      "source": [
        "class Model:\n",
        "    @staticmethod\n",
        "    def build(inshape=(28, 28, 1)):\n",
        "        inp = K.Input(shape=inshape, dtype=tf.float32, name='input')\n",
        "        \n",
        "        # Primary capsules\n",
        "        # For each digit in the batch\n",
        "        # 32 maps, each 6x6 grid of 8 dimensional vectors\n",
        "        \n",
        "        # First Conv layer\n",
        "        conv1_params = \\\n",
        "        {\n",
        "            \"filters\": 256,\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 1,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu,\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv1_params, name=\"conv_layer_1\")(inp)\n",
        "        \n",
        "        # Second conv layer\n",
        "        conv2_params = \\\n",
        "        {\n",
        "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
        "            \"kernel_size\": 9,\n",
        "            \"strides\": 2,\n",
        "            \"padding\": \"valid\",\n",
        "            \"activation\": tf.nn.relu\n",
        "        }\n",
        "        x = K.layers.Conv2D(**conv2_params, name=\"conv_layer_2\")(x)\n",
        "        \n",
        "        # Reshape\n",
        "        x = K.layers.Reshape(\\\n",
        "                             (caps1_n_caps, caps1_n_dims),\\\n",
        "                             name=\"reshape_layer_1\")(x)\n",
        "                             \n",
        "        x = SquashLayer(name=\"caps1_output_layer\")(x)\n",
        "        \n",
        "        x = MyDigitCapsLayer(name = \"caps2_predicted\")(x)\n",
        "        caps2_predicted = x # Save this value for later\n",
        "        \n",
        "        #routing by agreement (2 rounds)\n",
        "        x = RoutingByAgreement(name=\"routing1\", round_number=2)(x)\n",
        "        \n",
        "        return K.Model(inputs=inp, outputs=x, name='my')\n",
        "    \n",
        "m = Model.build()\n",
        "print(m.summary())\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw3j_zWQMlPZ"
      },
      "source": [
        "# y_train_train = tf.one_hot(y_train, depth=caps2_n_caps, name=\"T\")\n",
        "# print(y_train_train.shape)\n",
        "# #print(y_train)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zxRXCYIz_HQ"
      },
      "source": [
        "class MyAccuracy(K.metrics.Metric):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyAccuracy, self).__init__(**kwargs)\n",
        "    self.acc_obj = K.metrics.Accuracy()\n",
        "    self.state = 0\n",
        "  \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyAccuracy, self).get_config()\n",
        "    config.update({\"acc_obj\": None, \"state\": self.state})\n",
        "    return config\n",
        "\n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def update_state(self, y_true, input, sample_weight=None):\n",
        "    if self.acc_obj is None:\n",
        "      self.acc_obj = K.metrics.Accuracy()\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    y_true = tf.reshape(y_true, (y_true.shape[0], ))\n",
        "    y_true = tf.cast(y_true, dtype=tf.int64)\n",
        "    self.acc_obj.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.acc_obj.reset_state()\n",
        "\n",
        "  def result(self):\n",
        "    return self.acc_obj.result()\n",
        "\n",
        "class MyReshapeLayer(K.layers.Layer):\n",
        "  def __init__(self, axis=-1, keep_dims = False,  **kwargs):\n",
        "    super(MyReshapeLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shapes):\n",
        "    pass\n",
        "\n",
        "  def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=True, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "  def call(self, input):\n",
        "    print('printing shapes ------------------- ')\n",
        "    EPSILON = 1.0e-9\n",
        "    print(input)\n",
        "    y_proba = self.safe_norm(input, axis=-2)\n",
        "    print(y_proba)\n",
        "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "    print(y_proba_argmax)\n",
        "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "    print(y_pred)\n",
        "    return tf.cast(y_pred, tf.int64)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(MyReshapeLayer, self).get_config()\n",
        "    return config"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohNmlixkrAek"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "comparison_metric = MyAccuracy()\n",
        "#checkpoint_filepath = \"/content/drive/MyDrive/Weights/weights-improvement-{epoch:02d}-{val_my_accuracy:.2f}.hdf5\"\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = \"/content/drive/MyDrive/WeightsMnist/11_best_weights1.hdf5\",\n",
        "        save_weights_only=True,\n",
        "        monitor=f\"val_{comparison_metric.name}\",\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "model_checkpoint_callback2 = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = \"/content/drive/MyDrive/WeightsMnist/11_latest_weights1.hdf5\",\n",
        "        save_weights_only=True,\n",
        "        monitor=f\"val_{comparison_metric.name}\",\n",
        "        mode='max',\n",
        "        save_best_only=False)\n",
        "log_csv = CSVLogger(\"/content/drive/MyDrive/WeightsMnist/11_mylogs1.csv\", separator = \",\", append = False)\n",
        "callback_list = [model_checkpoint_callback, model_checkpoint_callback2, log_csv]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mb3hJfK4YnA",
        "outputId": "039b9e2f-f4eb-4f84-8403-c034b8776781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "comparison_metric.name"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my_accuracy_17'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZFYZzGt4YXz"
      },
      "source": [
        ""
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cahMgtOqMlPa"
      },
      "source": [
        "m.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "#history = m.fit(x_train, y_train, batch_size=32, epochs=2, verbose= 1, validation_split=0.2, callbacks = callback_list)\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bnBQabNNKLH",
        "outputId": "60486f24-3fed-49de-c591-f43efed39e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "print(f'Best Validation Accuracy = {np.max(history.history[\"val_my_accuracy_3\"])}')\n",
        "print(f'Best Training   Accuracy = {np.max(history.history[\"my_accuracy_3\"])}')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-8b89d1f385f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Best Validation Accuracy = {np.max(history.history[\"val_my_accuracy_3\"])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Best Training   Accuracy = {np.max(history.history[\"my_accuracy_3\"])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pOcuSJGSvLY",
        "outputId": "d56424ef-0a67-469e-932e-fe2ec0b2a44a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#m.save(\"/content/drive/MyDrive/WeightsMnist/save.tf\", save_format='tf')\n",
        "\n",
        "# \n",
        "!rm -f \"/content/drive/MyDrive/WeightsMnist/save3.tf\""
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/WeightsMnist/save3.tf': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SM-gwDR2FtA"
      },
      "source": [
        "m.save(\"/content/drive/MyDrive/WeightsMnist/save3.tf\", save_format='tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLeqoVJkjPKB"
      },
      "source": [
        "#Extra layer for evaluate\n",
        "class DimensionCorrection(K.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "      super(DimensionCorrection, self).__init__(**kwargs)\n",
        "    \n",
        "    def safe_norm(self, input, axis=-2, epsilon=1e-7, keep_dims=False, name=None):\n",
        "      squared_norm = tf.reduce_sum(tf.square(input), axis=axis,\n",
        "                                     keepdims=keep_dims)\n",
        "      return tf.sqrt(squared_norm + epsilon)\n",
        "\n",
        "    def call(self,y_pred):\n",
        "      y_proba = self.safe_norm(y_pred, axis=-2)\n",
        "      y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
        "      y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
        "      return y_pred\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3s5KL2po5FN"
      },
      "source": [
        "y_test = tf.cast(y_test, dtype= tf.int64)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cJcH3YlsAVt"
      },
      "source": [
        "def ignore():\n",
        "  m = Model.build()\n",
        "  m.load_weights('/content/drive/MyDrive/WeightsMnist/11_latest_weights1.hdf5')\n",
        "  m.compile(optimizer='Adam', loss=MarginLoss)\n",
        "  newmodel = K.models.Sequential(\\\n",
        "              [\\\n",
        "              m,\\\n",
        "              DimensionCorrection(),\\\n",
        "              ]\\\n",
        "  )\n",
        "  newmodel.summary()\n",
        "  m.trainable = False\n",
        "  newmodel.compile(optimizer='adam')\n",
        "\n",
        "  y_pred = newmodel.predict(x_test)\n",
        "\n",
        "  import sklearn\n",
        "  from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(f\"accuracy = {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJLicHf3J1G-"
      },
      "source": [
        "mm = K.models.load_model('/content/drive/MyDrive/WeightsMnist/save2.tf',\\\n",
        "                         custom_objects=\\\n",
        "                          {\\\n",
        "                              \"SquashLayer\": SquashLayer,\\\n",
        "                              \"SafeNorm\": SafeNorm,\\\n",
        "                              \"MyDigitCapsLayer\": MyDigitCapsLayer,\\\n",
        "                              \"RoutingByAgreement\": RoutingByAgreement,\\\n",
        "                              \"MyAccuracy\": MyAccuracy,\\\n",
        "                              \"MarginLoss\": MarginLoss,\\\n",
        "                          })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbZNCJZkZ46"
      },
      "source": [
        "# y_pred_eval = DimensionCorrection()\n",
        "# m2 = Model.build()\n",
        "# m2.load_weights('/content/drive/MyDrive/WeightsMnist/latest_weights1.hdf5')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-vE6yGAoBPI"
      },
      "source": [
        "# m3 = K.models.Sequential()\n",
        "# m3.add(m2)\n",
        "# m3.add(y_pred_eval)\n",
        "# m3.build()\n",
        "# m3.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "# m3.evaluate(x_test, y_test, batch_size= 32, verbose= 1)\n",
        "\n",
        "# #m3.evaluate(x_test, y_test, batch_size= 32, verbose= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vf-xeFUcgi"
      },
      "source": [
        "#converter = tf.lite.TFLiteConverter.from_keras_model(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuULPnldermd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdrYq9SwUr-N"
      },
      "source": [
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.target_spec.supported_types = [tf.float16]\n",
        "# quantize_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWt9eE4hpaDy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Vm_QpbUry8"
      },
      "source": [
        "# mm = K.models.load_model('/content/drive/MyDrive/WeightsMnist/save2.tf',\\\n",
        "#                          custom_objects=\\\n",
        "#                           {\\\n",
        "#                               \"SquashLayer\": SquashLayer,\\\n",
        "#                               \"SafeNorm\": SafeNorm,\\\n",
        "#                               \"MyDigitCapsLayer\": MyDigitCapsLayer,\\\n",
        "#                               \"RoutingByAgreement\": RoutingByAgreement,\\\n",
        "#                               \"MyAccuracy\": MyAccuracy,\\\n",
        "#                               \"MarginLoss\": MarginLoss,\\\n",
        "#                           })\n",
        "# print(type(mm), mm.summary())\n",
        "# print(mm.weights[0][0][0][0][0:5])\n",
        "# e = mm.load_weights('/content/drive/MyDrive/WeightsMnist/latest_weights1.hdf5')\n",
        "# print(mm.weights[0][0][0][0][0:5])\n",
        "\n",
        "\n",
        "# Create the .tflite file\n",
        "# tflite_model_file = \"/content/drive/MyDrive/WeightsMnist/compressed.tflite\"\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(mm)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# converter.target_spec.supported_ops = [\n",
        "#   tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "#   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "# ]\n",
        "\n",
        "# tflite_model = converter.convert()\n",
        "# with open(tflite_model_file, \"wb\") as f:\n",
        "#     f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1c026FiF3Hb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVrB1GkTFItQ"
      },
      "source": [
        "!du -sh /content/drive/MyDrive/WeightsMnist/*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTzAAQHpL51b"
      },
      "source": [
        "# Use this tutorial for pruning\n",
        "quantization has already been done earlier\n",
        "\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUQtzXdBqPh6"
      },
      "source": [
        "PRUNING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAVk1mxJqykT"
      },
      "source": [
        "pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_DKdw7nqBI5",
        "outputId": "fccea7a5-d574-4424-fd68-716a76cf0301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# TODO: Remove this line\n",
        "mm = m\n",
        "print(\"ORIGINAL MODEL\")\n",
        "print(mm.summary())\n",
        "print('-' * 80)\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = x_train.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "model_for_pruning = prune_low_magnitude(mm, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "\n",
        "model_for_pruning.summary()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "######################################################################\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "# Helper function uses `prune_low_magnitude` to make only the \n",
        "# Dense layers train with pruning.\n",
        "def apply_pruning_to_dense(layer):\n",
        "  print(\"called\")\n",
        "  if isinstance(layer, MyDigitCapsLayer):\n",
        "    print(\"\\n\\n\\nFound it ----- \\n\\n\\n\")\n",
        "    return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
        "  return layer\n",
        "\n",
        "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
        "# to the layers of the model.\n",
        "model_for_pruning = tf.keras.models.clone_model(\n",
        "    mm,\n",
        "    clone_function=apply_pruning_to_dense,\n",
        ")\n",
        "\n",
        "model_for_pruning = K.models.Sequential(\\\n",
        "              [\\\n",
        "                model_for_pruning,\\\n",
        "                MyReshapeLayer(),\\\n",
        "              ]\\\n",
        ")\n",
        "\n",
        "\n",
        "#model_for_pruning.compile(optimizer='adam', loss=MarginLoss(), metrics=[MyAccuracy()])\n",
        "model_for_pruning.compile(optimizer='adam', loss=K.losses.SparseCategoricalCrossentropy(), metrics=[K.metrics.Accuracy()])\n",
        "print(model_for_pruning.summary())\n",
        "\n",
        "model_for_pruning.fit(x_train, y_train,\n",
        "                  batch_size=32, epochs=2, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL\n",
            "Model: \"my\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv_layer_1 (Conv2D)        (None, 20, 20, 256)       20992     \n",
            "_________________________________________________________________\n",
            "conv_layer_2 (Conv2D)        (None, 6, 6, 256)         5308672   \n",
            "_________________________________________________________________\n",
            "reshape_layer_1 (Reshape)    (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps1_output_layer (SquashLa (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "caps2_predicted (MyDigitCaps (None, 1152, 10, 16, 1)   1474560   \n",
            "_________________________________________________________________\n",
            "routing1 (RoutingByAgreement (None, 1, 10, 16, 1)      11520     \n",
            "=================================================================\n",
            "Total params: 6,815,744\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "--------------------------------------------------------------------------------\n",
            "called\n",
            "called\n",
            "called\n",
            "called\n",
            "called\n",
            "\n",
            "\n",
            "\n",
            "Found it ----- \n",
            "\n",
            "\n",
            "\n",
            "called\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "printing shapes ------------------- \n",
            "Tensor(\"Placeholder:0\", shape=(None, 1, 10, 16, 1), dtype=float32)\n",
            "Tensor(\"my_reshape_layer_17/Sqrt:0\", shape=(None, 1, 10, 1, 1), dtype=float32)\n",
            "Tensor(\"my_reshape_layer_17/ArgMax:0\", shape=(None, 1, 1, 1), dtype=int64)\n",
            "Tensor(\"my_reshape_layer_17/y_pred:0\", shape=(None, 1), dtype=int64)\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "my (Functional)              (None, 1, 10, 16, 1)      8290306   \n",
            "_________________________________________________________________\n",
            "my_reshape_layer_17 (MyResha (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 8,290,306\n",
            "Trainable params: 6,815,744\n",
            "Non-trainable params: 1,474,562\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "printing shapes ------------------- \n",
            "Tensor(\"sequential_17/my/routing1/mul_3:0\", shape=(None, 1, 10, 16, 1), dtype=float32)\n",
            "Tensor(\"sequential_17/my_reshape_layer_17/Sqrt:0\", shape=(None, 1, 10, 1, 1), dtype=float32)\n",
            "Tensor(\"sequential_17/my_reshape_layer_17/ArgMax:0\", shape=(None, 1, 1, 1), dtype=int64)\n",
            "Tensor(\"sequential_17/my_reshape_layer_17/y_pred:0\", shape=(None, 1), dtype=int64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-fc920768dd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m model_for_pruning.fit(x_train, y_train,\n\u001b[1;32m     72\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                   callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:797 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:155 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:259 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1713 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4945 sparse_categorical_crossentropy\n        epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:940 _constant_to_tensor\n        return constant_op.constant(x, dtype=dtype)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py:457 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py:337 _AssertCompatible\n        (dtype.name, repr(mismatch), type(mismatch).__name__))\n\n    TypeError: Expected int64, got 1e-07 of type 'float' instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9aZWk2D5Bh",
        "outputId": "a1c50613-f930-4b2f-b9c3-5f6857b94dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7MOJdAOcud"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}